# IA: Embeddings y RAG

- <https://pro.codely.com/library/ia-embeddings-y-rag-230838/655241/path/>
- <https://github.com/CodelyTV/ai-search_engine_with_rag-course>
- Asesor: √Ångel Delgado
- Duraci√≥n: ~ 3 horas

## üöÄ ¬øQu√© es RAG y los embeddings?, ¬øqu√© aprender√°s en el curso?

- **RAG (Retrieval-Augmented Generation)**
- 3 big LLM problems
  - Hallucinations
  - Cut-off date (no saben nada anterior a cierta fecha)
  - Finite context
- 3 ways to **RAG** (mecanismos para filtrar el conocimiento que le paso al prompt y limitar el contexto)
  - `SELECT * FROM courses WHERE category = 'DDD'`
  - Full-text search
    - e.g. con ElasticSearch, todos los cursos que contienen la palabra "Intro"
  - Vector search
    - e.g. "l√≠stame los cursos introductorios de arquitectura y dise√±o de software". No contiene la palabra "intro" necesariamente, e.g. el de "Clean Code".
    - Soluci√≥n: b√∫squeda vectorial aka b√∫squeda sem√°ntica.
    - `SELECT * FROM courses ORDER BY (embedding <=> ${generateEmbedding("Intro a Arq. y dise√±o de software")}) LIMIT 10;`
- RAG is a technique that enhances the capabilities of LLMs by **integrating them with external knowledge sources** to provide more accurate, relevant, and up-to-date responses. 

### üßÆ Qu√© es una b√∫squeda sem√°ntica y qu√© son los embeddings

- Gato, gato negro, perro, perro negro
- Una dimensi√≥n (tipo de animal):           vector(1)
- Dos dimensiones (tipo de animal y color): vector(2)
  - E.g. El gato est√° en la [-1, 1], el gato negro en la [-1, -1]
- Un **embedding** es un vector de N dimensiones (N es del orden de cientos o miles).
  - Esas dimensiones, inicialmente, ten√≠an connotaciones de significado concreto. A d√≠a de hoy no, son conceptos abstractos.
  - Potencia de 2
  - A m√°s dimensiones, m√°s lenta es la b√∫squeda
- Se mide la "cercan√≠a" teniendo en cuenta el √°ngulo del vector desde lo que buscas a lo que existe (NO por distancia euclidiana)
- Existen **modelos de embeddings**: le pasas un texto y te lo pasa a un embedding, e.g. "perro muy negro"
- La **comparaci√≥n** del embedding se har√≠a en la propia base de datos, pero la **generaci√≥n** puede ser en la DB o llamando a un LLM o...
- **Embedding models** (to generate the embedding/vector)
  - `nomic-embed-text` vector(768): te lo puedes montar en local con OLlamA, <https://ollama.com/library/nomic-embed-text>
  - `text-embedding-3-small` vector(1536): de OpenAI
  - `text-embedding-3-large` vector(3072): de OpenAI
- [Recomendaci√≥n de Supabase](https://supabase.com/docs/guides/ai/vector-columns#create-a-table-to-store-vectors), recomienda vector(384)
  - Para el embedding usan el [modelo `gte-small`](https://huggingface.co/Supabase/gte-small): en la tabla aparecen las dimensiones.

### üßë‚Äçüè´ Implementa RAG b√°sico con PgVector + Langchain: B√∫squeda de cursos similares

- [C√≥digo de ejemplo: RAG con PgVector](https://github.com/CodelyTV/ai-search_engine_with_rag-course/tree/main/01-what_is_rag/3-implement_rag_pgvector)
- `pgvector` es la extensi√≥n de Postgres que permite hacer b√∫squedas vectoriales dentro de Postgres
  - NO se encarga de generar el embedding/vector, sino de calcular la distancia. Y tambi√©n permite almacenar el vector.
- A tu tabla normal, e.g. `cursos`, se le a√±ade un campo `embedding vector(768)`
- En este ejemplo, s√≥lo usa el nombre del curso para generar el embedding (usando OllamaEmbeddings, ofrecido por LangChain)
  - Tenemos que pasar el embedding a JSon antes de guardarlo en la tabla.
- De momento, una b√∫squeda por "Introducci√≥n a DDD" no est√° fina.

## üî§ Embeddings: B√∫squeda de curso por significado

### üê≥ Las diferentes formas de a√±adir embeddings

- En algunos tutoriales se propone tener una tabla con todos los embeddings, no tenerlos en las propias tablas de negocios.
  - Pero es m√°s sencillo empezar por a√±adir el campo `embedding` a la tabla
- Las diferentes formas de a√±adir embeddings
  - **Specific field**
    - A la tabla `courses` le a√±ado un campo `embedding`
  - **Specific table**
    - Tengo una tabla`course_embeddings` (desnormalizaci√≥n)
    - Probablemente la mejor.
  - **Generic table** (con todos los embeddings dentro)
    - Tengo una tabla gen√©rica `embeddings`
    - IMO demasiada abstracci√≥n, demasiada generalizaci√≥n

![](ai-embeddings-and-rag-codely/embeddings_table_comparisons.png)

### üì§ Contexto: C√≥mo es una aplicaci√≥n EDA con sugerencias LLM

- Overview del curso "Integra IA siguiendo buenas pr√°cticas"

### ‚ûï A√±ade RAG a una aplicaci√≥n EDA existente

- [Code example: add RAG existing embedding](https://github.com/CodelyTV/ai-search_engine_with_rag-course/tree/main/02-embedding/3-add_rag_existing_application)
  - The first execution failed because we were sending too much context. We can solve it using RAG.
- With RAG we limit the courses for which we want to generate recommendations
  - [We would ask for "similar" user courses, not all the user courses](https://github.com/CodelyTV/ai-search_engine_with_rag-course/blob/c966d5ac6c5d21d6eafbc7932a36a7ec3fcf61b4/02-embedding/3-add_rag_existing_application/src/contexts/mooc/user-course-suggestions/infrastructure/OllamaLlama31CourseSuggestionsGenerator.ts#L34-L34)
  - [`searchSimilar()`](https://github.com/CodelyTV/ai-search_engine_with_rag-course/blob/c966d5ac6c5d21d6eafbc7932a36a7ec3fcf61b4/02-embedding/3-add_rag_existing_application/src/contexts/mooc/courses/infrastructure/PostgresCourseRepository.ts#L75-L75)

```javascript
const availableCourses = await this.courseRepository.searchAll();
// const availableCourses = await
// this.courseRepository.searchSimilar(completedCourseIds);
```

- No hay determinismo, podr√≠a fallar: implementar por ejemplo _retries_

## üë®‚Äçüíª Optimiza tu RAG a√±adiendo m√°s contexto

### üõñ Qu√© valores guardar como embedding

- [Ejemplo de c√≥digo: qu√© guardar en el embedding](https://github.com/CodelyTV/ai-search_engine_with_rag-course/tree/main/03-add_rag/1-what_to_store_embedding)
- A la hora de crear el embedding, [a√±adimos m√°s informaci√≥n](https://github.com/CodelyTV/ai-search_engine_with_rag-course/blob/c966d5ac6c5d21d6eafbc7932a36a7ec3fcf61b4/03-add_rag/1-what_to_store_embedding/src/app/scripts/search-courses-by-ids.ts#L30-L30): guardar no √∫nicamente el nombre del curso, sino tambi√©n un resumen y las categor√≠as. De esa manera, obtengo una informaci√≥n m√°s precisa.
- [LangSmith](https://www.langchain.com/langsmith): observabilidad existente en LangChain
  - LangSmith is a unified observability & evals platform where teams can debug, test, and monitor AI app performance ‚Äî whether building with LangChain or not.

### üó£Ô∏è A√±ade m√°s contexto en los embeddings: B√∫squedas granulares

- [C√≥digo de ejemplo: c√≥mo a√±adir m√°s contexto en los embeddings](https://github.com/CodelyTV/ai-search_engine_with_rag-course/tree/main/03-add_rag/2-add_embedding_context)
- Para el embedding, [a√±adimos el ID, nombre, summary y categor√≠as](https://github.com/CodelyTV/ai-search_engine_with_rag-course/blob/c966d5ac6c5d21d6eafbc7932a36a7ec3fcf61b4/03-add_rag/2-add_embedding_context/src/contexts/mooc/user-course-suggestions/infrastructure/OllamaLlama31CourseSuggestionsGenerator.ts#L108-L108), en formato `yaml` (porque cuantos menos caracteres metamos para el embedding, mejor, si no se creer√° que importan - mejor no usar json o XML para esto)

### ‚è≥ Ordena los resultados de una b√∫squeda por embeddings

- Interesa hacerlo por ejemplo si queremos los N cursos similares m√°s recientes, por orden.
- [C√≥digo de ejemplo: ordenaci√≥n RAG m√°s reciente](https://github.com/CodelyTV/ai-search_engine_with_rag-course/tree/main/03-add_rag/3-rag_sort_by_recent)
- [En la SQL, ordenamos por la combinaci√≥n del embedding y la recencia (con un peso, 0.001, por prueba y error, se puede empezar por 0.1)](https://github.com/CodelyTV/ai-search_engine_with_rag-course/blob/c966d5ac6c5d21d6eafbc7932a36a7ec3fcf61b4/03-add_rag/3-rag_sort_by_recent/src/contexts/mooc/courses/infrastructure/PostgresCourseRepository.ts#L86-L86)

## üî• Qu√© base de datos elegir para hacer b√∫squeda por embeddings

### ‚õìÔ∏è C√≥mo delegar a infraestructura la generaci√≥n de embeddings

- [C√≥digo de ejemplo de pgai](https://github.com/CodelyTV/ai-search_engine_with_rag-course/tree/main/04-what_db_to_choose/1-pgai)
- C√≥mo delegar la l√≥gica de la evoluci√≥n de los embeddings.
- Uso de [pgai Vectorizer](https://github.com/timescale/pgai): Power your RAG and Agentic applications with PostgreSQL
  - No es lo mismo que pgvector. `pgai` es una capa por encima de pgvector. Es otra extensi√≥n que [se tiene que habilitar](https://github.com/CodelyTV/ai-search_engine_with_rag-course/blob/c966d5ac6c5d21d6eafbc7932a36a7ec3fcf61b4/04-what_db_to_choose/1-pgai/1-simple_example/databases/0-enable-pgai.sql#L1-L1)
  - [Docker container con postgres_pgai](https://github.com/CodelyTV/ai-search_engine_with_rag-course/blob/c966d5ac6c5d21d6eafbc7932a36a7ec3fcf61b4/04-what_db_to_choose/1-pgai/1-simple_example/compose.yml#L3-L3)
- Vamos a generar los embeddings autom√°ticamente: cada vez que se haga un insert, se genera su embedding.
  - Nos apoyamos en el [pgai-vectorizer_worker](https://github.com/CodelyTV/ai-search_engine_with_rag-course/blob/c966d5ac6c5d21d6eafbc7932a36a7ec3fcf61b4/04-what_db_to_choose/1-pgai/1-simple_example/compose.yml#L15-L15)
    - Cada 5 segundos mira si hay algo que actualizar. En Producci√≥n no ser√≠a as√≠: hay un Kafka de por medio y se enchufan a un stream de datos.
    - El worker necesita algo para generar los embeddings. En este ejemplo, con Ollama.
  - Es necesario [configurar a `ai` c√≥mo se deben generar los embeddings](https://github.com/CodelyTV/ai-search_engine_with_rag-course/blob/c966d5ac6c5d21d6eafbc7932a36a7ec3fcf61b4/04-what_db_to_choose/1-pgai/1-simple_example/databases/2-add_vectorizer_to_courses.sql#L1-L1)
    - Configuramos que el campo importante es `summary`
  - Tabla `courses_embedding_store`: la crea pg ai y no la tocamos directamente.
    - Lo que s√≠ usaremos ser√° la vista `courses_embedding` que se ha creado tambi√©n autom√°ticamente
- Creamos el embedding de la query [durante el SELECT](https://github.com/CodelyTV/ai-search_engine_with_rag-course/blob/c966d5ac6c5d21d6eafbc7932a36a7ec3fcf61b4/04-what_db_to_choose/1-pgai/1-simple_example/src/app/scripts/search-courses.ts#L15-L15)
  - Te quitas dependencias

### ü•ä Qu√© base de datos vectorial elegir: AWS vs Pinecone vs Postgres vs Supabase vs Timescale vs Redis

- pgai no depende de Timescale. Es decir, podr√≠amos usar pgai en cualquier base de datos Postgres.
- Postgres + pgvector (e.g. AWS Aurora, Supabase) VS Postgres + pgai (e.g. Timescale) VS Private (Pinecone, AWS Kendra) VS SqLite vec VS Mongo (Mongo Atlas) VS Redis (Redis Labs)
- Opini√≥n de opci√≥n m√°s sensata: **Postgres + pgvector**
  - Para embebidos: SQLite vec

![](ai-embeddings-and-rag-codely/vector_database_table_comparisons.png)

### üê¶‚Äçüî• C√≥mo a√±adir RAG utilizando una base de datos sin soporte nativo a tipos vectoriales

- [C√≥digo de ejemplo: combine databases](https://github.com/CodelyTV/ai-search_engine_with_rag-course/tree/main/04-what_db_to_choose/3-combine_databases)
- E.g. si usas MySQL. Podr√≠amos almacenar los embeddings pero NO hacer la b√∫squeda por distancia vectorial
- "Soluci√≥n": [guardar en Postgres √∫nicamente los embeddings](https://github.com/CodelyTV/ai-search_engine_with_rag-course/blob/c966d5ac6c5d21d6eafbc7932a36a7ec3fcf61b4/04-what_db_to_choose/3-combine_databases/src/contexts/mooc/courses/infrastructure/MySqlCourseRepository.ts#L55-L55)

## üçΩÔ∏è Ingesta en base de datos vectorial datos de terceros
- Veremos
  - Ingesta de PDFs
  - Ingesta de PDFs con la API declarativa
  - Scraping web

### üìÑ Ingesta PDFs a tu IA para hacer RAG con LanchChain

- [Ejemplo de c√≥digo: importa PDFs con LangChain](https://github.com/CodelyTV/ai-search_engine_with_rag-course/tree/main/05-3r_party_data/1-pdfs)
  - Versi√≥n **imperativa** con la librer√≠a de LangChain.
  - Usa pgvector, no pgai
  - http://localhost:3012/
  - Importamos PDFs de un directorio (usando LangChain), generamos `documents` y a partir de ellos los embeddings y los almacenamos en un [vectorStore](https://github.com/CodelyTV/ai-search_engine_with_rag-course/blob/c966d5ac6c5d21d6eafbc7932a36a7ec3fcf61b4/05-3r_party_data/1-pdfs/src/app/scripts/scrape-posts.ts#L28-L28).
  - El embedding se genera a partir del [content](https://github.com/CodelyTV/ai-search_engine_with_rag-course/blob/c966d5ac6c5d21d6eafbc7932a36a7ec3fcf61b4/05-3r_party_data/1-pdfs/src/app/scripts/scrape-posts.ts#L45-L45)
- LangChain permite formas sencillas de escrapear documentas, generar embeddings, buscar embeddings, etc.
  - Como un SQL.
- Usamos [rlm/rag-prompt](https://smith.langchain.com/hub/rlm/rag-prompt): 
  - No tenemos que picar nuestra query de prompt, [esto nos proporciona el prompt](https://github.com/CodelyTV/ai-search_engine_with_rag-course/blob/c966d5ac6c5d21d6eafbc7932a36a7ec3fcf61b4/05-3r_party_data/1-pdfs/src/app/scripts/search-posts.ts#L23-L23)
    - El `context` que se le pasa es el texto del PDF que haya encontrado haciendo RAG. Esto hace RAG por debajo para saber qu√© pedazo me tiene que traer.
- `npm run scrape-posts`
- `npm run search-posts -- "En qu√© minuto se habla de d√≥nde publicar eventos de dominio"`

### üó£Ô∏è Importa PDFs con la API declarativa de LanchChain
- Versi√≥n **declarativa** con la librer√≠a de LangChain
- [C√≥digo de ejemplo](https://github.com/CodelyTV/ai-search_engine_with_rag-course/tree/main/05-3r_party_data/2-declarative_pdfs)
- Se simplifica, ya no es necesario dos llamadas diferentes. Por otra parte, el uso de `RunnablePassthrough()` hace que no haya que pasar la `query` dos veces como hac√≠amos en la primera versi√≥n imperativa.

### üï∏Ô∏è Scrapping web con Playwright y LangChain
- [C√≥digo de ejemplo: web scrapping](https://github.com/CodelyTV/ai-search_engine_with_rag-course/tree/main/05-3r_party_data/3-web_scrapping)
- Uso de LangChain: [webpages, with Playwright](https://js.langchain.com/docs/integrations/document_loaders/web_loaders/web_playwright/): how to parse data from webpages using Playwright. 
  - No est√° pensada para usarla recursivamente
  - No podemos usar `RecursiveUrlLoader` porque no interpreta JS a la hora de escrapear.
- Usamos el `chromium` de Playwright, no el de LangChain, porque este √∫ltimo no permite tener varias sesiones abiertas a la vez (paralelismo).  
- http://localhost:3012/
- Si en la tabla de DB intentas meter un UUID que no es v√°lido, LangChain no peta y en su lugar te lo inserta creando un UUID: cuidado con eso para evitar insertar filas duplicadas.
- La respuesta tarda demasiado porque hay demasiado contexto. Troceando mejorar√°.

## üçÑ Mejora la b√∫squeda por embeddings con datos de terceros

### ‚úÇÔ∏è Formas de cortar los datos para guardarlos de forma optimizada: Chunking
- 4 ways of chunking
  - **Length-based**
    - Character-based: cada N caracteres
    - Token-based
  - **Text-structured based**
    - Por p√°rrafos: se puede limitar el tama√±o de los p√°rrafos
    - Paragraphs > sentences > words > chars
  - **Document-structed based**
  - **Semantic meaning based**

## üîú Conclusiones y siguientes pasos

- [ChatGPT Prompting vs RAG vs fine tuning: Aprovecha la IA al m√°ximo | #laFunci√≥n 9x18](https://www.youtube.com/watch?v=bjCdsnkQ6Dw)
- M√°s cursos
  - [Embeddings autom√°ticos en Postgres](https://pro.codely.com/library/embeddings-automaticos-en-postgres-236271/702554/about/)
