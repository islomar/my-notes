# Outside-In TDD

Examples of Outside-In TDD course of Pluralsight with Mark Seemann

Course URL: [https://app.pluralsight.com/library/courses/outside-in-tdd/](https://app.pluralsight.com/library/courses/outside-in-tdd/)

The course teaches how to build an application from the outside in - starting with tests targeting actual features or use cases of an application, but gradually working towards a more and more detailed specification of the components of an application. The focus is on the technical side of TDD, not the business side. Approximately half of the content is a series of C# demos, building a small RESTful service from scratch.


**Mark Seemann**
[http://blog.ploeh.dk](http://blog.ploeh.dk)

**IDE**
http://blogs.msdn.com/b/vscode/archive/2015/11/17/announcing-visual-studio-code-beta.aspx

## WALKING SKELETON

Walking skeleton: a thin slice of working functionality.

### BDD vs Outside-In TDD

- You can do BDD and Outside-In TDD at the same time.
- Overlapping but not mutually exclusive concepts.
- BDD: nace del PO. Create executable specifications.

* The agile testing quarant (Brian Merrick).

### Outside-In vs Bottom-Up

- Ouside-In:
    - London School of TDD
    - Behaviour Verification
    - Mockist
- Bottom-Up
    - Classic TDD
    - State-Based Verification
    - Triangulation

Comentarios

- ¿Usar sufijo Model? Ufff...
- Tests sin estructura clara Given-When-Then.

## SPIKING
Spike: clavo, pincho. Se usa el símil como analogía a atravesar algo end-to-end, desde fuera hasta dentro, llegando hasta el final (e.g. la DB).

Tests should be FIRST:

- Fast
- Isolated
- Repeatable
- Self-validating
- Timely: it should be written before writing the production code

Three phases of a test:

- Arrange
- Act
- Assert

Four phase test:

- Fixture setup
- Exercise SUT
- Result verification
- Fixture teardown

Backdoor manipulation (skip the SUT and access DB directly):

- Pre-populate the DB in the fixture setup.
- To access DB directly to verify the expected result.

## TRIANGULATION
Triangulation = Validation
Triangulation: typical from Kent Beck, Uncle Bob, for algorithmic SUTs.

Movement to rename TDD to Example-Driven Development (=triaunglation).
As the tests get more specific, the code gets more generic (Uncle Bob).

Meta-syntactic variable: https://en.wikipedia.org/wiki/Metasyntactic_variable
foo, bar, baz, qux, quux, corge

Devil’s advocate: only add enough code to pass the test (e.g. hardcoding the result returned) >> pair programming.
Gollum style: when you apply the “devil’s advocate” working on your own instead of pair programming.

Doubt: webtoken, no creo que lo haya hecho con outside-in, primero creó el WebToken con triangulación y después añadió los tests externos.

## BEHAVIOUR VERIFICATION
Measure side effects.
Decrease permutations.

Characterization tests (Michael Feathers): it is written after the system under test code. It captures a snapshot of its visible external behavior.
It characterises the current behaviour of the SUT. Tests written a posteriori.

“ceteris paribus”: https://en.wikipedia.org/wiki/Ceteris_paribus >> "with other things the same" or "all or other things being equal or held constant"


## DISCUSSION
http://app.pluralsight.com/courses/discussion/outside-in-tdd


## REFERENCIAS
Jason Gorman: Classic TDD or London School?
[http://codemanship.co.uk/parlezuml/blog/?postid=987](http://codemanship.co.uk/parlezuml/blog/?postid=987)


## TRANSCRIPT

Walking Skeleton

**Introduction**

Hello. My name is Mark Seemann, and this is the Outside-In Test-Driven Development course, Module 1, Walking Skeleton. In this module you'll learn how to build a walking skeleton by applying Outside-In Test-Driven Development. This may not make a lot of sense to you right now, but once the module has completed, it should. In order to get you there, I'll give you a brief introduction to the concepts involved, including the concept of a walking skeleton. Also briefly talk about positioning Outside-In Test-Driven Development against other types of software testing, just to clear out any misunderstandings that might arise. Once we've discussed all those concepts, I'll show you a demonstration of building a walking skeleton using C#.

This is not a TDD introduction

This course is not an introduction to test-driven development in general. There are Pluralsight courses that introduce test-driven development called Test First Development, Parts 1 and 2. This course assumes that you're familiar with the content of those courses. If you haven't seen those courses, you might want to do that now. If you haven't seen those courses, but you're familiar with test-driven development from other courses or from experience, that's probably going to be just as good.

Purpose of Outside-In TDD

Outside-In TDD is valuable because it helps us to align what we're doing as software developers with what the product owner wants the software to actually do. We do that by start testing at the boundaries of the system. So that might be at the user interface or at a service layer, like a RESTful service or SOAP service. Because that's most closely aligned with what the product owner typically wants. They want a new feature, and they can express that in terms of using interfaces or in terms of service features that the service endpoint should have. And this helps us apply a principle called YAGNI -- you aren't going to need it -- because we as developers have a tendency to gold plate our code, if we don't know what it is that we're actually going to build. But by start building from the outside and going inwards, we tend to only write the code that we're actually going to need, in order to meet the requirements that we've stated for ourselves by writing the test at the boundaries of the system. And that again helps us to bring better business value.

**Outside-In TDD at a glance**
Here's a high-level perspective of how Outside-In TDD works. Just like with normal TDD, the first thing you have to do is write a failing test. That's the red box here. And obviously it's red because it's currently failing. Just like with TDD, now that we have a failing test, we have to produce the real system. This is what we call the system under test, henceforth known as the SUT. We have to produce the system, in order to make the test pass. Now, what normally happens with this first test, when doing Outside-In TDD, is that this prompts you to write quite a bit of infrastructure code. So I'll try to illustrate that by making the SUT box slightly larger than the test itself. But still, we're trying to write just enough code to make the test pass, and nothing more. I've also tried to illustrate that we're only testing against external boundaries with a slightly smaller blue box in front of the SUT. Now, since this first test is passing, we can now write a new failing test against the same external boundary. And in order to make that test pass, we must slightly expand the SUT, creating a little bit more code, in order to make both tests pass. Another point we should be aware of is that we're not constrained to only test against the external boundary that we started out testing against it. We can also introduce new external boundaries, where we write test against those, as well as the ones that we already had. So we can keep on iterating and writing new tests against any of those external boundaries. And each time we do that, we slightly expand the SUT by writing new code to meet the requirements that each of those tests encapsulate.

**Testing means many things**
Testing means many things to many different people. So, just in order to clear out any misunderstandings and make it more specific what it is that we are talking about, I'll talk a little bit about outside-in TDD in relationship to behavior-driven development, in relationship to the agile testing quadrants, related to the test pyramid, and finally related to bottom-up test-driven development.

**Isn't this simply BDD?**
The first question you may have is, isn't this simply the same as behavior-driven development? Behavior-driven development is a testing discipline where you work tightly with the product owner to specify a set of executable specifications that you can then use to validate whether the software that you're building is actually meeting those requirements or not. And those executable specifications are usually executed against the boundaries of the system. So that sounds a lot like outside-in test-driven development. And in fact, you can definitely do BDD and outside TDD at the same time. But those are overlapping and not mutually exclusive concepts, because you can do BDD in other ways than outside-in TDD. And you can also do outside-in TDD without explicitly doing BDD. In this course I'm not doing BDD, because I'm writing the outside-in TDD tests as developer tests, and I don't have any product owner or customer with whom I'm tightly collaborating with. I could have let it do that, but it's not a requirement to do outside-in TDD.

**The agile testing quadrants**
There are many ways to think about software testing. You may have seen this diagram before. It was originally introduced by Brian Marick and talks about the many different ways that we can think about testing. In this course, we're focusing mostly on the left-hand side of this diagram, because as developers, and when we do test-driven development, we tend to focus on unit tests or component tests or things where we can quickly run the tests and get a yes or no answer back, whether we are actually fulfilling the requirements or not.

**The test pyramid**
The test pyramid is a concept developed by Mike Cohn, and it describes how automated tests should be distributed among different types. So at the user interface level, you should only have a few automated tests. And the reason for that is that even though the product owner tests care about the user interface, such tests are very hard to develop and maintain, and they tend to be very brittle. They also typically run rather slowly. At the service level -- and some people would call this the subcutaneous level -- you can have more automated tests here, because it tends to be more robust, but still quite slow. And you should have most of your automated tests at the unit testing level, because, if done correctly, unit testing is actually a pretty robust way of doing automated testing. So, what happens in relationship to outside-in test-driven development is that you start at the top of the pyramid here. But then you work your way down. Which means that you write a few automated tests at the service boundaries of your system. But then, to flesh out the behavior of the system, you work your ways into the bottom of the pyramid here. And you should end up having most of your automated tests at the unit testing level.Further modules in this course will explore the concept of going from service boundary level tests to unit tests and fleshing out the system with unit testing.

**Testing at boundary and unit levels**
Another way of looking at that is, if you recall this diagram from before, you have a couple of tests that tests at the boundaries of the system. But then you can have tons of more tests that tests the internals of the system at the unit testing level.

**Outside-In or Bottom-Up**
Overall, there are two different approaches to TDD: Outside-in and bottom-up. These also go by a lot of other names, such as the London School of TDD versus Classic TDD, behavior verification versus state-based verification, and the mockist versus triangulation styles. The point that's important to me to make is that these aren't mutually exclusive approaches. So even though we work from an overall approach of outside-in TDD, we can still do triangulation or state-based verification. One of the modules in this course actually talks about how to do triangulation from within the overall process of doing outside-in TDD. If you want to learn more about the difference and similarities of outside-in versus bottom-up, Jason Gorman has a very nice article called "Classic TDD or London School".

**Walking Skeleton**
The first thing you should do with outside-in TDD is to build a walking skeleton. That's basically the outcome of writing the first test and making it pass. So you could say that a walking skeleton is the artifact of writing that first test. The book {italic}Growing Object-Oriented Software, Guided by Tests, {plain}defines a walking skeleton as an implementation of the thinnest possible slice of real functionality that we can ultimately build, deploy, and test end-to-end. The reason why we are talking about a skeleton is that at this point this system may not actually do anything particularly interesting. It just sits there and makes happy noises. One thing to be aware of with this definition is that we can use it to drive various aspects of the software, not only just the coding.We can use this definition to drive the infrastructure of our system. So this can help us pick those technologies that make it as easy as possible for us to create and deploy the software that we're building. So this helps us pick one Web framework over another Web framework, or one persistence technology over another persistence technology, based on how easy is it actually to test and deploy the system. It also helps us to define the process, not only TDD itself, but also how do we deploy the system.

**Technical constraints**

A typical process that we would like to arrive at is to automate as much as possible, not only the testing, but also the deployment. Ideally we should be able to just pull the source from our source control system and run the tests without having to do anything else. So we want to be able to assume that there's no previous environment setup required, that you don't have to create special directories or that you don't have to set up special environment variables or registry keys or other things on the system itself. This is very important in a team environment or when you have a continuous integration system, because you have multiple environments and you don't want to have to maintain and synchronize the settings of all those environments. So you want to automate those things by putting it into the source that you pull from your source control system. Also, we would very much like not to have to run as administrator,partly because that's not what the real system is going to do, but also because that's another environment thing that we have to think about. So the only things we can really assume is that we would have general tools, such as Visual Studio, for example. And that's the only thing we can really assume from the process. And everything else should be automatable.

**Demo introduction**

Now that you understand the principles and theory behind outside-in TDD and a walking skeleton, it's time to see how to do that in practice. So in this demo, I will show you how to write a walking skeleton, using Visual Studio and C#. The scenario will be a REST API that captures information about a user's exercise runs. So, the user can go for a run, and then they can upload a journal entry that captures how far they went when they did that and how fast they ran. And then later on they can go to the same REST service and review the journal entries that they already made. And the first thing to build a walking skeleton is simply to write a test and make sure that there's something responding on the other end. So this is the first test that I'm going to write, and it's going to be very, very simple and a little bit boring, because it only tests that something sits on the other end of the service boundary -- in this case, a network address -- and actually responds to the requests that we're giving it.

Demo: Getting a response from a Walking Skeleton

Building a walking skeleton means starting from scratch. The only thing that I have so far is just a solution called RunningJournalAPI and a single project for my service boundary test, those coarse-grained tests that I'm going to write at the boundary of the system. And just to show you that I haven't really done anything in preparation besides that, these are just the default references that any library project comes with. So the first thing I need to do is to pick a unit testing framework, or an automated testing framework. And you can pick whatever automated testing framework that you like. I particularly like the one called xunit, but you could also have picked end unit or another favorite automated testing for your work that you like. Now it's time to write the first automated tests. And I'll do that by adding a class called HomeJsonTests. And the reason why I call it that is because, if you think about the root resource of a RESTful API as the equivalent of the home page for a -- for a web-based application. And the reason why I called it json is because I'm going to use json as the serialization format going back and forth. So the first test I want to write is just a simple test that tests whether the response returns a correct status code. So remember, at this point I'm just writing a test that pretty much verifies that there's a system at the other end and it's actually listening to the requests that we are sending. In order to invoke the external service boundary of the service, I need a client. And I want to use this new HTTP client class that's new for .net 4 and 4.5. The problem is that I don't have it in my references at the moment. It's not something that ships with the BCL as such, so I need to add -- add it as a nuget package. So I can do install package. Again, it's called microsoft.net.HTTP. And we're just downloading this one. And now we can go back to the test and we'll see now we have system.net.HTTP and we can add a reference to that. From the client I can now get a response, which is what I really care about. So I'll just get the root resource here and block until the result comes back. And the only thing I really care about at this point is just to see whether the response was some sort of success status code. And if not, I could write out the actual status code, like this, and that's going to help us a little bit when we troubleshoot. And there we have it. And let's see that actually compiles. And at this point I can try to just run the test here. And we shouldn't really expect it to work at this point, and certainly it doesn't. There are lots of things that we need to fix about this test, before we can actually make it work. But this test, as it currently looks, represents the essentials of what it is that I currently want to test. But we need some more infrastructure in place to actually make this work. So I'm using a trick that I originally heard from Kent Beck, who said that if you're in doubt about how to express the test that you're writing, try to write it backwards and figure out what's then going to be necessary for you to add to that test, in order to make it work. So, right now I'm just going to use the exception messages that I get back from the system, in order to inform me of what I need to add to this test, in order to make it work. The first exception is an invalid operation exception. And it says, An invalid request URI was provided. The request URI must either be an absolute URI or its address must be set. And if we look at the line where that exception was thrown, we see that it's actually been thrown at this line and we never really reached the assertion. So, there's something that we need to fix, in order to make this exception go away. What the exception tells us is that either I should provide an absolute URI at this point or I should set a base address on the client. So let's look at setting that base address on the client. That takes a URI. And I'm actually going to a variable that we can call baseAddress as well. And that obviously doesn't exist, so let's put it up here. It's a baseAddress that's going to be new URI of -- let's see -- HTTP localhost. And just some port. It's not really very important at this point what that address is. But we can try to run the test again and see whether it helped with that particular exception that we got. This is a new exception. And when we dig into the -- all the inner exceptions, we see that we get a classic no connection could be made, because the target machine actually refused it on that specific IP address and port. And that shouldn't come as a surprise to us, because there is no service listening at that particular address and port at the moment. So this should prompt us to actually create something that's listening at this specific address. And this is where I might be tempted to set up IIS and create a service that's listening on this specific address. But that would go against a constraint that I should be able to take the source code for this whole system and pull it from the source control from some -- some other machine and just have the system running. So I can't have that specific environment setup that requires me to have IIS configured in a certain way. Luckily, the ASP.net Web API, which is a new framework for building RESTful services, allows me to do in-process hosting of the service that I want to ride. And this is one of the reasons why I think it's very interesting to pick that particular technology over other alternatives, because it allows me to do in-process hosting. And it actually means that I can host the service from within the specific test case, and I don't even have to run as administrator to do that. So in order to do that I'll have to look at the options that I have here with the HTTP client. And if you look at the constructor overloads that this one gives us, this one that allows us to provide an HTTP message handler. And the ASP.net Web API has a self-host feature that actually implements the HTTP message handler abstract base class. In order to add that, I'll have to go out and install the package. It's called microsoft.aspnet.webapi.selfhost. So I'll add that package, and it's going to download a lot of things. And I'll just wait until that's done. And what I can do now is that I can create a new server. And I'll say that's going to be a new HTTP self-host server and just pull in that one. And that derives from that HTTP message handler class that I can put into the HTTP client here. And on the other hand, you will see that I have squiggly lines now. And that's -- this is because there is no default constructor for the HTTP self-host server, and I'll need to provide an HTTP self-host configuration. So again, I'm just letting the current state of my test inform me of what I need to do next. So I'll do a new HTTP self-host configuration and pass that into the server here. So I'll just pass in the config instance. I have the same problem with the HTTP self-host configuration is that it needs a base address. I can provide a URI for that. So that's why I put the base address in a variable up there. And now we can try to run the unit test again -- or the acceptance test, that is, and see whether -- what actually happens. This exception actually tells us that we've made progress. Because now we get a message back saying, Actual status code NotFound. And the test failure actually comes from the assertion itself. So no longer do we have an exception being thrown by the client, but we actually have the assertion evaluating to false in this case. And the status code is NotFound, which indicates to us that there's something listening on that particular address, but there's no resource with that particular URI that we requested. And in order to fix that, we need to create the system. The ASP.net Web API basically works by dispatching incoming requests to controllers. And the way it does that is by using this configuration system here. So we can look at the HTTP self-host configuration -- actually, its base class HTTP configuration -- and see there's a collection of rules here which encapsulates the rule for how incoming requests are actually going to be dispatched to specific controllers. So, what we have at the moment here is we have a config class that's basically empty.And we pass that to the self-host server at the moment. So we need to have something happening in between those two lines of code that configure the root. And I prefer that to be the system itself that has that responsibility. So we could imagine that the system itself has this bootstrap class and it has a configure method. And we can pass in that config instance into the config -- into the configure method. And once that's done, we can start serving requestsby just patching into specific controllers. Now, this obviously doesn't exist yet, so we'll have to create the project for the system under test. Now I'll go and add to the new project. And this can just be a normal library project, and we'll call that RunningJournalAPI. And we'll just get rid of Class1. And then we can use the refactoring tools of Visual Studio to create the new type. And we want to type to be in the RunningJournalAPI project that we just created. And likewise, the configure method, we can add that by creating a method stub. Now, if we go to that method, we can see that we're still missing something,because we don't have a reference in that new project to the Web API. So we'll have to go over to the new get package manager console and install the package to the new project here. So we'll just do -- let's see we if have it in -- we can -- in history. We don't need the self-host, but we just at the call. And I'm just adding this to the RunningJournalAPI. And it doesn't have to download as much, because we already had that before. So this means we can now get rid of all the namespace stuff here. And we don't really want to reference the self-host configuration, because we only need that base class that we had before, which is defined in the call library. And we'll just get rid of that exception there that's throwing. So we'll try to run the unit test again on the -- accept this test. And we can see that nothing really happened. We still have an actual status code of NotFound, because basically, at the moment, this is just a method that does actually nothing. But now we have the building block in place to configure the system and create a controller forward. In order to configure the system, we can add a root to this config instance here. And basically, what we say is that if the controller segment of the URI is empty, we will default to a controller called the JournalController. So let's add a class called the JournalController. And in order to be a controller for the Web API, it needs to implement an interface called IHTPController. But we can also do that. The easiest way to do that is just by deriving from a class called APIController.Now we can try to run the test again, to see how far we've got. And we can see here that we actually get a new status code MethodNotAllowed. Before it was NotFound, but now it's actually saying method -- MethodNotAllowed. And informs us that we've gone a little bit further, because now there's actually something listening at this specific address. It's just telling us that HTTP get requests are not allowed. The journal controller must handle the get requests,and we do that by adding an action method. And there are various conventions around how an action method should be defined. One of the return types that we can give it is an HTTP response message. And there's a convention about the naming, as well, saying that if you want to handle get requests, the name should start with Get. So I could have said GetDefault or GetHome as well. But Get is fine. And I'll just return the response here and be done with that.So, now I can actually run the test again. And you will see nothing really popped up. But I have one Passed and zero Failed or zero Skipped down in the corner. Just manually show the output here and we'll see that everything is actually passing at that moment.

**Demo recap**

So let's recap what happened. This is a walking skeleton that I'm building. So I just want the simplest possible test to get something up and running. So in this case I'm doing a get request against the rude resource of my system. And the only thing that I'm really getting back is just 200 okay. There's no body in the HTTP response at all, but that's okay for now. However, if we think about the definition about a walking skeleton, the idea about building a possible slice of real functionality is probably a little bit of a stretch at the moment, because we just have something that responds. So maybe we should think about doing just a little bit more before we can call it a day. The next thing we can think about is to post a journal entry, because that would later on allow us to read back from that first resource and see whether we could read what we posted.

Demo: Posting an entry

Here's the same Visual Studio solution as before. The only changes I've made since the last demo is that I've moved some of the test infrastructure code to a helper class. So you can see that in this create method here I have all the self-host configuration and the self-host server sitting here, as well as this idea about the base address. But if we look at the journal controller, it's the same as before. And if we look at the bootstrapper, it's exactly the same class as before. So nothing has really changed. However, it's important to notice that when you write code and when you write test code with TDD, it's important to treat your test code as a first-class citizen, because this is something that you're checking into your source control system and you have to maintain it, just as you have to maintain your real code. So the same principles for maintainable code apply here, as well as they do in the system under test code. What I need to do now is just to make a post against a service. And that's pretty similar to making a get against the service. In the first instance I just want to test whether the response is still a success status code. Obviously, I'm going to do something else than actually doing a get. And we'll also need to change the name of the method here. So we can just call it PostReturnsResponseWithCorrectStatusCode. First of all, we need something to post, and I want to post json as the running journal entry. And one way to model json is to just create an anonymous type. So you can see json here is just the type quote a, which hasn't been defined yet. And the journal entry should include a time, which could just be DateTimeOffset Now. We'll also need to log a distance, and that could be 8,500 meters. And a duration, which is a TimeSpan. And in this case we could just do fromMinutes 44. And now that we have this anonymous type, we can look at the response when we ask the client to do a post. So we'll do post as json async and post the json. And then we're just going to block and wait until the result comes back. Let's run the test and see what happens. So, we'll run the test and we'll expect it to fail. And it -- certainly it does fail. We get the actual status code as MethodNotAllowed. And we see that the failure of the test is actually in the assertion itself. So that's not too surprising, because the journal controller doesn't allow posts at the moment. It only allows HTTP gets. Now, in order to make this second test pass, we can pretty much just do what we did before and change the method name to Post. Let's run both tests and see whether they succeed or fail. And we'll see that they both succeed, and none of them fail, and none of them are actually skipped.

**Demo recap**

To recap what just happened, it was pretty much the same as in the previous demo. Instead of doing a get request, I now do a post request and post some case json to the service. And just as before, the only thing that really happens is that I get a 200 okay response back. No message body, no nothing. So, at this point, you may be thinking that I'm cheating a little bit, because I haven't really added any functionality to the service at all. I'm just checking out the boundaries of the system and seeing whether they actually respond or not. So, in order to add this thinnest possible slice of real functionality that a walking skeleton requires us to do, the next thing I can do is try to post a journal entry and then make a get request against that same resource and see whether the posted entry occurs amongst the entries that I get back. So that could close the loop and provide us with the walking skeleton that we require.

**Demo: Posting and reading back an entry**
This is the same code that we left behind in the previous demo. And if we look at the journal controller, it should be pretty evident to us that there's something missing here, particularly because we do a post, but we don't really save the value that's being posted at all. So the value that's being posted is simply thrown away. In order to fix that, we need to write one or more tests that will prompt us to add the desired functionality. I write a new test. And I can use an existing test here as a template. I'll do a copy and paste and just change the name of the test method to better capture the intent of this particular test case. And I'll also just add or change some of the values a little bit to better distinguish between different test cases. And the json that I post, I need to remember that as the expected value to look for. And I'm just going to convert that to a json.net object, because that's what I read, when I read it back out.So, I can post the json and just wait for the call to return. And then I can get the response from a subsequent get request to that same resource. So that gives me the response back. And in order to make an assertion, I need to read the actual json content out of the response. So I'll do a Content.ReadAsJsonAsync and just wait for the result of that to come back. And with that, I can make an assertion that it -- it contains and I'm looking for the thing that I expected. And the thing that should contain the expected thing is actual entries. And you will notice this is a dynamic property, because we're using json.net. I run the test, to see what happens. We don't expect it to succeed at the moment. And notice that I get a NullReference exception from the test. And it turns out, if I debug that, that this is because the content is currently null. When you look at the journal controller and the get method, it's not so surprising that the content is null, because I'm currently using an overload of the CreateResponse method that doesn't return any value. So I can use this other overload here that requires me to provide a status code. And I can just go okay. And the value I will return will be a model, like in the model view controller sense of the word. And I'll just create this new JournalModel class to capture the model of the content that I'm going to return. Running the test again, I should now expect that the NullReference exception from before is no longer there. But instead, the assertion failure that I'm looking for should be right here, and it is. Assert.Contains Not Found. And also reports the json that I was expecting, but that the test didn't find. Now that I have a test that fails in the expected way, it's time for me to go into the post method and capture the json that I actually posted to the method. And I'll do that by creating a new model class called the JournalEntryModel, which will be used to model the specific entries. And we can aggregate those into the JournalModel. And I need someplace to store this. So for now I'm just going to do a static list and add that journal entry to that static list. And I can now use that static list to read from that static list and add to an Entries property on the journal model itself, so I'll just do Entries.ToArray here. And obviously that Entries property doesn't exist at the moment, so I'll just add that property to it. Once again, I run the tests and get the same assertion failure as before. And you obviously figure out what's missing now, I can set a breakpoint and I can run the tests in debug mode, in order to figure out what the return values actually are. And we can see that there's something coming back. There are some entries coming back, but these look like they are empty. So we need to fix that, in order to continue. The reason the return entry is empty should be clear, if we look at the JournalEntryModel that we used to capture the posted json. That model has no properties, so it doesn't capture any of the data elements of the posted json. So I'll need to add time, distance, and duration properties to the model, in order to capture that data. Now I can run the test again. It still fails at the same point as before. I can use the debugger to step into the code and have a look at the actually returned data and see whether we are near the goal. And now we can see we actually have time, distance, and duration values in the entry that we get. The only remaining thing that I need to deal with now is to fix the casing from Pascal casing to Camel casing, because that's what the test wants me to do. That's easy to do with the Web API, because it's just a configuration change. So I go to my bootstrap class, in order to change the configuration of the json formatter, setting the resolver to something called the CamelCasePropertyNameContractResolver. And now, when I run all the tests, they all pass.

**Demo recap**

In this demo, like in the previous demos, I still tested the external boundaries of the system, first by making a post request, posting some json that represented a journal entry into the system, and just got a 200 okay response back. Then subsequently, I made a get request against the same resource and got back a 200 okay response, with an Entries array, including the entry that I just posted.

**Did I cheat?**

You may think that I cheated a little bit, because instead of using a real persistence engine, like a database or a file, I just used a read-only static list of journal entry models. And certainly, this is not the final state that I wish to arrive at. So in the next module, I'll show you how to spike a solution that uses real persistence technology, such as a database.

**Summary**

In this module you learned that Outside-In TDD is one among many testing techniques. It favors testing of external system boundaries over testing the internal functionality of the system, at least at the beginning. But still, keep Mike Cohn's test pyramid in mind. So while we start at the top of the pyramid by testing the external boundaries of the system, once we expand on the system, we'll write more and more unit tests to flesh out the behavior of the system.The demo that I showed in this module demonstrated how to create a walking skeleton with the ASP.net Web API.

**Spiking**

Introduction

Mark Seemann: Hello. My name is Mark Seemann, and this is the Outside-In Test-Driven Development course Module Two, Spiking. In this course you'll learn about the concept of spiking itself. You'll also learn about a concept called FIRST, which is actually an acronym that describes some attractive prophecies that your unit test should have when you do test-driven development. You'll also learn about fixture, setup and teardown, which are part of a pattern called Four Phase Test. All of those things are important when we look at Outside-In Test-Driven Development and this concept of spiking. And to pull it all together, I'll show you a couple of demos with Visual Studio and C-sharp that demonstrate how to apply these principles.

Application perspective

With Outside-In TDD you don't need to have a detailed upfront plan for your applications architecture. There's nothing wrong with having a sense of direction that you want to go in, in terms of the applications architecture. But initially, it often helps to just think about the applications architecture as a box,and then see what happens when you implement the application in response to the automated test that you write against the external boundaries of the system. This is called Emergent Design, and helps you to adhere to this YAGNI principle -- you aren't going to need it -- and helps you to only write the code that you actually need to write in order to meet the requirements of the product owner. The external boundaries describe features of the application such as the user interface or a riskful service or maybe the application to even receive messages and sends messages over MS's bus. But the application of stakeholders also have other concerns for the application than just what can be observed over the external boundary of the system. One of these is the ability to store data in a persistent stall such as a file system or a database that the application itself owns. This means that the application also has some internal boundaries. This is where it communicates with its persistent stalls. So far when we've talked about Outside-In TDD and the concept of a walking skeleton in Module One, what we did was just poke around at the external boundaries of the system. So you might recall the demo from Module One, where I did a very simple feature by posting a running journal entry to the service and just reading it back. And you might recall that I never really saved the data in any sort of persistent stall, but I just kept in memory and returned it from memory. This is probably not what we really want to do because we want to make sure that the data that's going to be posted is actually going to be saved in a persistent stall. This means that not only do we have to interact with the external boundaries of the system, but we also need to interact with the internal boundaries of the system to make sure that the data that we are putting into the system is actually going to be saved in that persistent stall. So you can think about that as going all the way through the external boundary and into the internal boundary to see whether that data was actually stored. And we call that spiking because it looks like we're driving a spike all the way through the application. We're not building the entire application, but we're going through the entire application for a very thin slice of functionality and making surethat we're going all the way through.

**FIRST**

Once we introduce persistence technologies to our automated tests, there are some things that we need to be aware of, that up until now hasn't really been an issue. To understand what those are, we have to take a detour around this acronym called FIRST, which are first sort of described by Tim Uttinger. The FIRST acronym here says that tests should be fast. They should run as quickly as possible. They should be isolated so that each test can run independently on any other test. They should be repeatable, which means that you can repeat the test over and over again and get the same result every time. They should be self-validating, meaning, that they will be able to tell by themselves whether they've passed or failed. And they should be timely, meaning, that they should be written at the correct time in the process, which means before you actually write the production code. When we talk about persistence technologies isolated and repeatable and specifically the characteristic of being isolated is very important, because when we talk about the persistence technologies, what happens is that we leave behind stayed on the system, and that might actually influence the tests that run after the test that left behind stayed on the system. So we have to clean up after ourselves.

**Four-Phase Test**

Not only do we have to clean up after every test, but we have to do so in a manner that guarantees as much as possible that clean up actually happens. So when in each test do we do that? You're probably familiar with this arrange act assert pattern for writing unit tests that says that each test should be divided into three phases. Here's one of the tests I wrote in the demo for Module One. The first phase is called the Arrange Phase, and this is where I set up all the things that I need to actually invoke the system. Next up, I just act on the system on the test that I have, in this case, calling client at GetAsync. And then in the third phase I assert that the response that I got back was actually as it was supposed to be. But you might notice that there's a implicit fourth phase in this test that happens when this using scope ends. What actually happens when the using scope ends is that dispose is called on the client, and this is the sort of cleanup that happens. So the question is how does this fourth phase fit into the arrange act assert pattern? And what do we call it? And it turns out that there's a different way to think about this arrange act assert pattern, is actually a superset of that pattern and that pattern is called Four Phase Test.And instead of calling it arrange act assert, we call those phases fixture setup, exercise SUT verification and fixture teardown. And you see that we have an explicit fixture teardown where we clean up everything that we created so far. And this is the part of the test where we need to go and clean up any persistent data that each test case left behind. We tend to just draw this four phase test as four boxes stacked on top of each other. And normally they will have equal size. But this is all part of a patent language introduced by Gerard Meszaros and his unit test patterns book back from 2007, which is a highly recommendable read. ( Pause )

Setting up and tearing down a database

When you want to introduce a persistence technology such as a database into your automated test, you need to make sure that any data left behind on that database is deleted again, and the safest way that you can guarantee that is by deleting the database after each test run. Using the patent language are the Four Phase Test. Here's how that might look like. So first, in the fixture setup phase, what happens is that you can actually create the database for every test case. This might seem like a lot of work. But we have modern computers nowadays and it's actually not that slow. Then after you set the database you can exercise the system on the test, getting your result verification. And then as the test completes, you can tear down the fixture again, which means simply deleting the database. You should, however, be aware that even though this runs pretty fast on modern computers, compared to a unit test where everything happens in memory, this is a rather slow process. So we don't want to have too many of our automated tests working in this way. And if you recall, the test pyramid by Mike Cohn, this also fits pretty well with the concept of the pyramid, that we don't want to have too many tests at the top of the pyramid. But the unit test that we can have, that's the base of the pyramid, shouldn't be using the database in this way. So we can minimize the number of automated tests that we have that actually touches the database, because setting up and tearing down this database for each and every test case will have a performance impact on how fast our test run.

Demo introduction

In this demo I will show you how to set up and tear down the database for each test case. There's one assumption here, and that is that the system is going to have SQL Server 2012 Express. And if you recall the discussion in Module One where I talked about making as few assumptions as possible of the environment. I'm not assuming that SQL Server 2012 have any particular settings or that any specific database actually exist. I'm just assuming that the software is installed and enabled on the system. So that's a fairly benign assumption that I think I can make. ( Pause )

Demo: database setup and teardown

Here's the running journal API solution that you saw in Module One in the demos, and it's pretty much where we left off at that point. It still has the in memory data stall of the journal entries. So what we don't need to do now is that we need to first set up and tear down a database for each test case so that we something that we can actually write to and read from. And why we could do that inline in each test case. We can also take advantage of the extensibility features of xunit.net and define that as an attribute that we can put in each test case. So we do that by deriving from this before after test attribute. And what that's going to do is it's going to give us some hook methods that will inform us when the test actually runs. So the before method here will run before the test case, and then the after hook method will be executed after the test case actually runs. And the good thing about this is that the test run actually guarantees that this after method will run even if an exception was thrown in the test itself. Why I could add code for setting up and tearing down the database directly in the attribute here, I rather prefer to delegate the work to a reusable class, like the bootstrap class that we already have. So I'm adding this installed database method to the bootstrap class, and that will contain the real implementation of how to actually set up the database. And we can also add in the same spirit an uninstall database that we can actually use to uninstall again. And the good thing about doing that is that now the bootstrap class contains all the information of setting up and tearing down the database, which means that we can reuse those same methods to implement, for example, and install on uninstall script for the application. In order to set up the database, I need to define the database schemer. And I think it makes most sense to actually define the schema and the language that the database itself speaks, that is, T-SQL. So I'll add a text file that can contain the database schema in the form of SQL statements. The first thing I need to do is to create the database if it doesn't exist. However, it might exist. So I need to query the master database to figure out if the database already exists and delete it if it does, and then I can go and create the tables that I need. I need a user table, and I need a table to hold the journal entries. This script is an important artifact of the application because a database administrator might want to get hold of it, or you might want to use it for deploying your application to the production environment. So in order to always make it available in the output directory, I'm just going to mark the file S, copy it to the output directory always. However, from the internal code of the system I don't want to reference the script as a file. That's too unwieldy in my taste. So instead, I'm going to add it as an embedded resource so that it can simply access the contents of this file as a string. To use this string of SQL statements I need to execute it against a database. And in order to connect to a database, I need a connection string. So I'll ask the configuration manager about the connection string that I need. And to do that, I'll need to add a reference to the system of configuration assembly. And given that, I can get the connection string for this particular connection. However, the connection string is likely to be given as a connection to the real database and that database doesn't exist yet. So I'll need to connect to the same machine but to the master database. So I'll need to pass the connection string and change the database name into the master database. Fortunately, there is a class that can pass connection strings for us and this is the SQL connection string builder. So I'll just pass that and change the name of the database into master. Now I have a connection string that I can use to create a new SQL connection object. And once I have the connection, I can open it. You may wonder why I use such basic data access technology as SQL connection and SQL command instead of using, for example, an OIM or some other database access technology. But keep in mind that what I'm doing here is not your normal CRUD operation. I'm actually trying to set up the database itself. And I simply think that it's easier to just use those basic tools than to use a complete OIM framework or similar for that. So now I can just take the database schema that's in my embedded resource as a string. And if you recall how that script actually looked, it contained a series of go statements. And a go statement is something that T-SQL doesn't really understand. That's a specific SQL Server T word. So I need to split the string over each of those go keywords and execute a query against each of those SQL statements in the right order. That's it for setting up the database. But I also need to write the code for deleting the database after each test run, and that's pretty similar. So I'll just take a copy of the -- or the connection code that I had and paste that into the uninstalled database method, and now I write the script for deleting the database. And I really don't need to reference an embedded resource here because deleting a database is basically just a question of querying the master database to figure out whether that database already exists. And if it does, just drop the database. And with that SQL statement I can create a new SQL command and execute the query for that command. This completes the setup and teardown of the database. So now it's time to run the tests. The test with the used database attribute fails with a Null Reference Exception, right here in this line of code, and that's because there's no connection string with the name of running journal; so I'll add one. This connection string is worth a bit of further examination. First of all, it has pooling set to false. That's unusual but required, because otherwise the ADO objects I just used are going to keep alive a connection to the database and that will prevent the uninstalled database method from dropping the database. Secondly, you can see that a connection string connects to localdb, v11. This syntax indicates that the database serverin question is the local machines instance of SQL Server Express 2012. No local database file is required which makes it all a lot easier. With the connection string in place, I can run all the tests and you can see that they all pass.

Demo recap

In this demo I created all the infrastructure required to set up the and tear down a database for each test case. I did that by creating the use database attribute, which can be used to adorn every test case where I want the database to be set up and torn down before and after each test case executes. I defined a SQL script that defines the schema of the database, and I also added a connection string to the automated test project.

Backdoor Manipulation

With all that infrastructure in place it's not possible to write automated tests that verify that the application correctly assist data to the database and reach data from the database. One possible technique we can use to do that is called backdoor manipulation. We'll use the fixture setup and fixture teardown phases as before to create and delete the database. We can also use the fixture setup of Phase 2, populate the database with the data that we need in order to run a specific test case. In the exercise SUT phase we'll still talk to the SUT directly. We can also in the result verification phase talk directly to the database and see whether the database was left in a state that matches our expectations. ( Demonstration )

Demo introduction

In the next demo you'll see how to use the backdoor manipulation technique to create a spike through the application. This test will demonstrate that the application can correctly read data from the database and return it through its external service boundary. The first part of the test we'll pre-populate the database with a bit of data. This is the backdoor manipulation part of the test. And this happens in the fixture setup phase of the test. Afterwards, the test will read the data through the SUT's external service boundary and verify that the return data matches what was pre-populated into the database. ( Demonstration )

Demo: spiking

To perform backdoor manipulation, I'll need to write code that writes data to the database. And in the SUT I'll need to read code from the database.Contrary to the setup and teardown of the database, this is within the well-known realm of CRUD. So instead of relying on low level ADO.net, there are better choices to pick a data access library. In .net the pure Microsoft solution is to use the entity framework, but I think that it creates too much friction in a TDD scenario. Many of the entity framework assigned time features are based on the assumption that there's a copy of the database available, and that's not the case here. The database only exist in short time frames while the test is executing. This makes it hard to work with the entity framework and its wizards. The entity framework also supports code FIRST development, but the consequence of that is that the actual database including its schema becomes a side effect of the code. In my experience that's going to create a lot of friction. It's better to have a single authoritative definition of the database schema. And I find that it only makes sense to define that in the database native language T-SQL. And that's why in the previous demo I already created the database schema. Since the database schema is already defined in T-SQL, any or M that relies on strong typing by a code generation is likely to have synchronization problems between code and the actual database. Therefore, I'd rather skip that sort of friction and write data access code without strong typing. Apart from ADO.net, there are various libraries available based on dynamic types. One of these NO resource project, called simple data, is a good match for the code application; so here I'm installing it by a Nuget. In the home JSON test class it's time to add the new test. Since proper fixture setup and teardown is required for this test, I add the use database attribute. The name of the test is going to be get root returns correct entry from database. The first step the test should take is to use a bit of backdoor manipulation to insert a journal entry into the database. The symbol data, this is done with a dynamic expandoObject instance where it can add time, distance and duration. These properties are going to be matched with column names in the database. In order to also remember what to expect from the SUT external service boundary, I create a snapshot and convert it to a J object, which is the Json.net type I also used to read the return Json. To save the journal entry I need to open a connection to the database, and to do that I need a connection string. Before, I added a reference to the system dot configuration assembly from the SUT project. Now I need to do the same from the test project. This enables me to create an object representing the database using the simple data API passing in the connection string. Since the database schema associates every journal entry with a user, first I must insert a user. I'll call it Foo for now and get the user ID of the new row. User ID is a foreign key in the journal entry table. And it's the last column value missing from the entry I'm about to insert into the database. So I assign that value to the dynamic object. This enables me to insert the entry in the journal entry table. The rest of the test is similar to the demos from Module One. I create an http client and make a get request, rate the response as Json and verify that the expected entry is contained within the returned entries. The test is now complete and I can run it. Since this is TDD, I expect the result to be read. And sure enough, the test fails, stating that the expected entry wasn't found in the return data. This prompts me to implement the desired feature. First, I must install a simple data in the SUT project, just as I did before in the test project. Next, I must change the implementation of the get method to pass the new test. This includes reading from the database. So once more, I need the connection string to open the database object. From the database I can now perform a query, setting all the journal entries belonging to the user with the user name Foo. The return title of the query's dynamic object, but a simple data provides an extension method called ToArray that convert the result into an array of the desired type, in this case, journal entry model. I assign the data from the database to the return value from the journal controls get method and run the new test to verify that everything is okay.Although the output window pops up, the test does indeed pass. The reason the output window pops up it's because simple data writes diagnostics information to the console and the testdriven.net test run sees that and pops up the window. Why this is the default behavior of simple data, it could be turned off in configuration. The important thing is that the test passes. But what about the rest of the test's read? When I run all the tests, it turns out that several other tests are now failing. This is because I changed the implementation of the SUT to expect a database to be present, but several of the existing test cases don't get set up and tear down the database. In order to fix this, I must add the use database attribute to all of the existing acceptance tests. This repairs most of the tests, but there's still a single failing test left. This is the test from the demos in Module One. You may recall that at first post an entry to the running journal risk ABI and then it tends to read it back with a get request. The reason is that the get method now reads from the database, but the post method doesn't write to the database. Obviously it should do that. To get started, I simply copy the database object initialization code from the get method and paste it in to the post method. The general entry must be associated with a user. So I insert a user with the name Foo into the user table. The insert method returns a user ID. And together with the values from the posted journal entry model, I can now insert a row into the journal entry table. At this point neither the post nor the get method uses the original in memory list of entries. So I can delete that and run the tests read again. This time all tests pass.I have successfully driven a spike through the application from the external boundary to the internal boundary. ( Demonstration )

Demo recap

In this demo you saw how to drive a spike through an application. I used simple data for both the backdoor manipulations and to implement the site itself.It's worth noticing that you don't have to work with the relational databases. You could instead just choose to use the file system or a document database.But in this example I chose to demonstrate how to work with a relational database system because that's typically a type of system that gives rise to a lot of challenges when working with TDD. And one of the ways that you can tackle a challenge like that is to pick the tool that matches your process. In this case simple data matches TDD very well.

Did I cheat?

In this module you saw how a spike implements a thin slice of functionality of an application all the way from the external boundary to the internal boundary.If you follow along in the demo, you might be left with a nagging feeling that maybe I cheated a little bit. Did I? I hardcoded the username Foo in two different basis in the SUT. Is that okay? It isn't okay, but that's all the test requires of me to do right now. So I'll have to write more tests to force me to do something that's more correct. In later modules I'll show you how that works.

Summary

This module looked at the concept of spiking which is a way to ensure that not only does the application appear to behave correctly at the external service boundary, but it also does the right thing at the internal boundaries. Spiking completes the walking skeleton. If you recall from Module One, the definition of a walking skeleton was a thin slice of real functionality. So even though in Module One we created some apparently correct behavior, you probably didn't have the correct real functionality because I never saved data into a database. The concept of isolated test requires us to deal with the fact that when we work with persistence technologies, we have to make sure that every test cleans up after itself. That leads us into the concept of fixture teardown. You really should be aware that setting up and tearing down a database after each test case results in a slow test read. And again, I'll remind you of test pyramid that says that you should only have a few of those tests, and then you can have in memory-based unit tests that comprise the bulk of your automated tests read.We also looked at the concept of backdoor manipulation where you go around the SUT and directly deal with the database in order to either test that it was left in the right state or in order to pre-populate it with data required for the test case. In the next modules we'll leave the external boundary and start flushing out some other parts of that test pyramid with unit tests created by triangulation and behavior verification.

Triangulation

Introduction

Hello, my name is Mark Seemann and this is the Outside-in Test Driven Development course, module three, triangulation. In this course you'll learn about triangulation which is one of two major approaches to unit testing. And I'll show you a demo of how that works. You'll also learn about a technique called devil's advocate which you can use with test driven development and I'll show you a demo of that.

Outside-In versus Bottom-Up

You may recall this slide from module 1 where I contrasted outside-in versus bottom-up test driven development. In this module I'll focus on bottom-up test driven development. There are various names for bottom-up test driven development such as classic TDD or state based verification or triangulation. Those names are mainly synonymous but I think that triangulation captures the essence of that way of testing very well. It's worth noting that outside-in TDD and bottom-up TDD are not neutrally exclusive.

When to use

Since there are two major approaches to unit testing you may wonder which one should you use when. Triangulation is sort of the classic test driven development technique. This is what you typically see when you see the original inventor of TDD Kent Beck demonstrate the technique. You also see it demonstrated by people such as Robert C. Martin and a common trait of those examples is that they tend to focus very much on systems under test that are algorithmic in nature such as a stack or a scoring system for a bowling game or something like that. Behavior verification on the other hand, can be used to measure side-effects of a system under test or it can be used to decrease the number of permutations that you would have to test for. In the next module I'll talk more about behavior verification and how that works but for now I'll focus on triangulation.

Stimulus/Response

In essence triangulation works by applying a stimulus to the system under test and then measuring the response. The response may take the form of a return value but might also be a change of an observable state of the system under test itself. Together we call return values and the observable state of the system under test again for direct output. That's what you measure when you verify the results of your test.

Triangulation

Triangulation is actually a technique from the physical world where we can use geometry to measure for example the height of the building or the position of a ship at sea. The first measurement you take only gives you the direction of the target that you want to measure so you only really know the direction of the target but you don't know the distance to the target. Now you move and take another measurement and the intersection of those two measurements now gives you an idea of the precision of the target. Now if you know the distance between the two places where you took the measurement you can use basic geometry to figure out the exact position of the target. In software development we can more or less just replace the terms of measurement and target with the test case and the system under test.

Sampling

Imagine for a moment that you are given a library to which you don't have the source code and you don't know what it does. Imagine, furthermore, that it has a function or a method that it takes a string as input and returns the string as output. In order to figure out what this method does you'll have to measure it. So this is like sampling where you sample the output of example input and form your hypotheses around that. So the first input you can try is just the empty string and it turns out that this input returns the empty string as output. At this point you might form a hypothesis that says that this method simply just returns the empty strings output and ignoring the input. In order to test that hypothesis you can try it with a different input. It turns out though that if you use the input age the output is also age. So the first hypothesis doesn't hold. Now you can form a new hypothesis maybe saying that the method simply returns the input as output. In order to test that you can, for example, try the input ll and it returns ll so at this point the hypotheses still seem to hold.However, you may not be entirely confident in that hypothesis still, so you decide to try an input that's not as symmetric as the previous inputs. So you try the letters HE and now the output is EH which indicates that the previous hypothesis doesn't hold. Now you're likely to form a new hypothesis that says that the method probably reverses the string and returns that as output. And in order to test that hypothesis you try the input HELLO and the output is OLLEH.At this point the hypothesis seems to hold and you may decide to do more sampling just to get a better confidence in your hypothesis. This is reminiscent of the scientific method. Triangulation is also very similar to this process, the only difference is that at the beginning there is no library. Rather you could say that the system under test will coalesce from the input and the expected output from your tests.

Test cases as examples

Test cases are samples of the desired functionality of the system under test. Similar to the sampling process I just described but with measurements taken before the fact. In science there's a concept known as the observer effect where the mere act of observation has an impact on the subject being observed.This is most prominent in the field of quantum mechanics but we can think of triangulation along the same lines. The observer effect is so strong that it produces the outcome with the intervention of a developer. A couple of years ago there was a movement to rename test driven development to example driven development. The main motivation was that many TDD pundits felt that the word test held back adoption because programmers didn't want to test.Today that seems less important but the term example driven development seems almost synonymous with triangulation. The process of sampling until we have gained confidence in our hypothesis is very similar to test driven development. Robert C. Martin coined the expression as the test gets more specific the code gets more generic.

Demo introduction

It's time to see triangulation in action. in this demo you'll see how adding test cases one at a time gradually causes the system under test to coalesce. We'll pick up where we left in the demos in module two but this time we drop down to the unit level. If you recall my calls test pyramid, most tests should be unit tests and indeed, in this scenario there's no need for a full systems test. The feature is going to be serialization of a simple Web token which is an open standout for authorization data commonly used together with the Olaf protocol.

Demo: Simple Web Token serialization

In the module two demos I made one major shortcut. I hard coded the user name as FOO. It's time to rectify that situation and in order to do that I'll need to add authentication and authorization information to the running journal API. The protocol OLAF and the token form of a simple Web token are good matches for RestFulle APIs. In this and the next demo I'm going to triangulate a simple web token class that can serialize and deserialize claims about the user such as the user name. A simple web token is an independent class implementing passing and formatting logic. So it's a great candidate for triangulation. There is no reason to perform a full system test so instead of sticking with the existing acceptance test project, I add a new unit test project to the solution and add an X unit library to the new project. A good TDD trick is to start with a so-called ice breaker test. A simple test that mostly serves as a trigger to create the set class. This simple Web Token class should encapsulate a set of claims so I'd like to verify that it's possible to read it as a sequence of claims. As the red squiggly lines on the simple Web Token class tells us, the class yet doesn't exist so I use the built-in refactoring tools to create the class in the production project. The test now compiles so I can run the test to verify that it's failing, as expected. To make it pass I must make the simple Web Token class implement by innumerable of claim. Now all tests pass. However when we consider the implementation the not implemented strongly suggests that we're not yet done. Another test is required to fix that. I'd like to verify that I can read the claims I add to a simple Web Token instance so I ran a test called set yields injected claims. First I must declare what I expect which is an array of three claims with the types and values FOO, BAR, BES, COOKS, COOGS and CORG. Secondly I pass these claims into the constructor of the simple Web Token class and thirdly I verify that the expected array is equal to the sequence produced by the set itself. Lastly since there's also a non-generic overload of the get enumerator method I add an assertion that verifies the exact same thing but expressed using the non-generic overload. Right now the test doesn't compile because the simple Web Token doesn't contain a constructor that takes one argument. This prompts me to add a constructor taking a param's array of claims. It doesn't break the existing test that used the default constructor because now the existing test will just compile into this constructor with an empty array. Now that the code compiles I run that test and notice that the new test fails catastrophically because of this not-implemented exception. While the test doesn't pass, this doesn't count as a proper failing testbecause the assertion is never evaluated. Before I write the correct implementation of the get enumerator method I must write a wrong implementation in order to verify that I wrote the correct assertion. Thus I return and empty sequence. When I run the tests I see that the test does indeed fail but now it failsas expected on this line of code. Now it's okay to add the correct implementation. First I added backing field for the injected claims and assigned them from the constructor. Secondly I returned the enumerator from the backing field. The test still failed because of the second assertion that hits the non-generic overload of get enumerator. Once more I must first implement it incorrectly to see that the assertion verifies the correct condition. It does because the test now fails as expected on the second assertion and only then can I correctly implement the method. The tests now pass. Both of the tests I just wrote verify the direct output of the set but there hasn't been a lot of stimulus. You can consider this a warm-up to the real triangulation which starts now. The simple Web Token class must be able to serialize itself into the correct format defined by the simple Web Token standard. A very common way to serialize and object to a string is to use it' true string method. So I write a test called two string returns correct results. The set is a simple Web Token instance with a crane with a type FOO and the value BAH. To apply stimulus I invoke the two string method and subsequently verify that the result was the string FOO equals BAH. The simple Web Token stand out defines the format as a series of name value pairs very much like a URL query string. The test fails as expected so I override the two string method to return the hard coded string, FOO equals BAH. This implementation passes all the tests. Obviously this isn't yet the correct implementation so I need to add more test cases to make the implementation more general. While I could do that by writing new test methods, part of the X unit library called X unit extensions contains support for parameterized tests. Instead of the fact attribute the parameterized X unit test uses the theory attribute. It enables me to add one or more inline data attributes which contain the test parameters. As test case input for the test case I want to extend, I need an array of claims but since I can only use constants and attributes I'll need to settle with supplying an array of strings that I can then pass over the pipe character. After the clean values I also supply the expected outcome of the test. These values are simply the values which are already hard coded in the unit test, I'm simply moving them to the inline data attribute. To capture the inline data values I add a string array parameter for the claim, keys and values and a string parameter for the expected result. In order to pass the clean values into the simple Web Token constructor I must convert the string array to claim instances. First I split each string over the pipe character, secondly I create a new clean instance from the resulting array. Finally I convert the sequence into an array and pass it into the simple Web Token constructor. I also replace the hard coded value FOO equals BAH with the expected parameter value. This is the same test case as before only expressed in a different way. The tests still pass. With the test method expressed as a parameterized test it's easy to add new test cases. The first test case I want to add is the degenerate test case where there are no claims. This should result in the empty string. As the test failure demonstrates that's not currently the case so I changed the implementation of the two string method using the any method and the turn of reconditioned to return the correct value in both test cases. now all tests pass once again. The two string implementation still doesn't seem general enough so I add a new test case with two claims. One with FOO as the claim type and BAH as the value and another with BAS as the claim type and CUX as the value. The expected results should be FOO equals BAH and BAS equal CUX. As expected the new test case doesn't pass. In the two string method I decide to implement the method using link so for each claim I select a string created from the claims type and value. If there are no claims at all the resulting sequence may be empty so I define the default as the empty string. This sequence of name value strings can finally be aggregated with the ampersand character. This implementation passes all the tests and looks correct to boot.

Demo recap

In this demo you saw how parameterized tests make triangulation easy by providing example values in inline data attributes. You also saw how a simple test case can be to a naive implementation and first with more examples does the system on the test solidify and approach the desired implementation.

Devil's Advocate. Motivation

You may also have been thinking that at times I almost went out of my way to implement the code as naively as possible. You know what? I did. This is actually a deliberate technique called Devil's Advocate. One of the challenges in TDD, especially for beginners is to only write enough production code to pass each test. Once they are working in the production code many beginners forget about the tests and continue working as a production code without adding new tests. The Devil's Advocate technique can be used to combat that tendency and helps to focus the unit test on the essence of what they attempt to accomplish. It's originally a pab programming technique but you can also use it in sole mode as a way to critique your own tests as you write them.

Red/Green/Refactor

You have probably seen the red green refractor cycle before. It's a way to describe TDD as an intritive process. First you write a failing test, secondly you add just enough code to make it pass. Finally you may refractor the implementation code. The Devil's Advocate technique helps you with the second step of writing just enough code to pass the test.

Ping Pong

The Devil's Advocate technique is a variation of another pair programming technique called Ping Pong which works like this. The first developer writes a failing test and passes the keyboard to the second developer. The second developer implements the system of the test so that the new test, together with all the existing tests pass. According to the red green refractor cycle, both developers can now collaborate on refracting the code but the keyboard stays with the second developer and now new functionality can be added. When refactoring is over the second developer writes a new failing test and passes the keyboard to the first developer. The first developer modifies the system on the test to make all tests pass. This cycle can be repeated as long as necessary.

Devil's Advocate

The Devil's Advocate variation on ping pong goes like this. The first developer writes a failing test and passes they keyboard to the second developer. The second developer make all tests pass but makes an extra effort to implement the new feature in a naive or even an incorrect way. Both developers can see this and as long as it's possible to write a simple but incorrect implementation that passes all the tests it indicates that more test cases are required. Keep in mind that we are in the overall context of triangulation. The second developer now writes a new test in order to provoke a better implementation of the system under test. The first developer now makes a similar effort to pass the test as naively or incorrectly as possible. If this is possible it may either mean that more test cases are required or that the test wasn't concise enough. Often this provides valuable feedback of the quality of the unit tests particularlyabout the result verification step. There comes a time when an incorrect solution would be more complex than a proper solution. You often identify that point as part of the refactoring phase in the red green refactoring cycle. When this happens it's time to move onto a new feature.

Gollum Style

Although the Devil's Advocate technique is based on bad programming you can also use it while working independently. All it requires is and ability to constantly shift your perspective in order to play both roles from the pab programming techniques. I call this technique Gollum style because it reminds me a bit of the internal dialog kept by Gollum. You start out as nice Gollum by writing a failing test and then you instantly morph into a nasty Gollum and pass the test as naively or incorrectly as possible. Rinse and repeat.

Demo introduction

In this demo you'll see more triangulation. This time with the Gollum style used more aggressively. The previous demo serialized a simple Web Token to a string and in this demo you'll see that converse feature passing a string into a simple Web Token instance.

Demo: Simple Web Token parsing

In order to pass a simple Web Token I'd like to add a try-parse method. So I write an icebreaker test called Try-parse enveloped string returns force just to get started. The stimulus to this test will be an invalid string with the value of FOO. This isn't a valid simple Web Token because it's not a name value pair.The Try-test coding idiom returns the passed value as an out parameter if passing is successful. In this test case I don't acre about that because I don't expect passing to succeed but I still need to define it to satisfy the compiler. In the pattern language of X unit test patterns this is called a dummy. The actual result is produced by invoking the Tri pass method with the invalid string and the dummy. Finally I assert that the actual result is false. The test throws an exception so first I must change the implementation to verify that I wrote the correct assertion. Since I wanted the result to be false there's only one proper way to fail the test and this is by returning true. Again to satisfy the compiler the method must also assign a value to the out perimeter. For now no will do. The test now fails in the expected manner. But the fix is simple, return false instead of true and all tests now pass. According to the red green refactor process I just completed the red and green steps and now it's time for the refactor step. The parameter names could certainly be better so I change the invalid string to a token string and dummy to token. The tests still pass. Robert C. Martin describes a technique called the transformation priority premise which quickly told provide abstract guidance on what the next test case should be in order to transform the code towards a better implementation.The most preferred transformation it's possible to perform on the current code, is to introduce a conditional so that it can return both false and true. The next test case must trigger this transformation so I add the test method try-test valid string returns correct results. The expected result of the test can be expressed as an array of claims. Recall that a simple Web Token instance is essentially an I innumerable claim. In the first test case I use good old FOO and BAH to produce the valid token string I can use the simple Web Token cause since previous tests have demonstrated that the two string method is correctly implemented. Some people would argue that this is no longer a unit test and semantically I'd agree with them. I call such a test a facade test. It's still a valuable and succinct way to express the test because I built it on work I've already done. The actual reliable is declared to capture the out perimeter and the try-test method is invoked with the valid string and the actual perimeter. To verify that the try-test method works as intended, the test first asserts that the return value was true. Next it compares the expected and actual claims. Because the claim class doesn't implement any custom equality it's necessary to add a test specific claim comparer. This class doesn't exist yet so I added to the unit testing project and make it implement I equality compare of claim. The equal's method should return true if both claims have the same type and the same value. The get hash code method can simply return zero. This isn't the most efficient implementation but it doesn't hurt correctness. The test now compiles so I can run it to verify that it fails as expected. To play Devil's Advocate I change the try-test method with as many hard coded values as possible. This is clearly not a good implementation but it passes both tests.Those hard coded values are clearly unreasonable so that next test case should attempt to get rid of at least some of them. As a first effort I'll focus on the case of invalid token strings. Instead of writing a new test method I refactored the existing test for invalid input into a parameterized test. FOO is still the invalid token string but now that I've added it as a test parameter I need to delete it from the test method body. This is the same test as before and it still passes. Another common invalid input is null. But that test case fails because the return value is true. Playing Devil's Advocate I insert a null check in the try-pass method. This passes all the tests despite being more complex than before. In a new attempt to get rid of some of the hard coded values I add another test case this time for a string containing only spaces. The test fails as expected, however my inner evil Gollum manages to fall in my plans for getting rid of any hard coded strings. Clearly I'm not adding the right test cases in order to move forward. A better approach is probably to add good old BAH as invalid input. While I could add yet another Boolean condition in order to return false this is becoming more complex than warranted. Right now the code is very specific about what constitutes invalid input and very forgiving about what constitutes valid input. This doesn't seem right so another option is to turn the Boolean check on its head. If the input is anything other than FOO equals BAH the input is considered invalid. This passes all tests and while the Devil's Advocate technique still left me with a hard coded string at least the cyclamatic complexity of the method was reduced. This takes care of invalid input so now it's time to focus on the branch with valid input. Clearly the code is yet incorrect. Instead of writing new test methods, once more I convert an existing test method to a parameterized test changing the fact attribute to theory. The way that I need to create instances of claims is very similar to a parameterized test I wrote in the previous demo. It's a bit above the top of the screen so I collapsed the test in between so that you can see both at the same time. First I want to reproduce the current test case as parameterized test so I covered the FOO BAH test case from the previous test and pasted itinto the test I'm currently working with. The expected string isn't needed in this test case so I delete it leaving only a string array. Like the previous test I add the string array perimeter case and values to the test method and use the same link query to produce the expected claims from the parameter. This is the same test as before and it still passes. This enables me to easily add more test cases such as the degenerate case of now claims at all. The test fails as expected and the best Devil's Advocate modification I can come up with is to explicitly check for the empty string and return an empty simple Web Token instance as well as true. It's far from pretty but it passes all tests. The next test case is two claims instead of one. I'll reuse the good old FOO BAH claim but add a second BAS COOKS claim to the test case. This fails as expected. At this time as a Devil's Advocate I can't come up with any more wrong code that would be less complex than a proper implementation so I decide to attempt writing a correct algorithm. First I get rid of the ridiculously narrow specification of correctness, but I realize that at the very least I must still return false on a null string. This is just as much a got class as anything else. Now I know that if I get passed that if statement the token string isn't null. Secondly the algorithm must check to see if the token string looks correct so it spits itout into name value pairs over the ampersand character. Each of these strings are expected to be a name value pair separated by the equal character. If that's not true for all of those strings than the method returns false. If the algorithm gets passed that check that assumes that the string was valid and that it can create claim instances from it. For each name value pair it splits the pair over the equals character and creates a new claim instance from the resulting array. Finally the token can be created as new simple Web Token instance with the claim sequence converted to an array. This passes all tests and looks like a correct implementation.

**Demo recap**

In this demo I used triangulation together with the Devil's Advocate technique to implement a passer for simple Web Tokens. Please be aware that this implementation isn't a full implementation of the simple Web Token standard and that it's not secure. At the very least is lacks a digital signature and an expiration time but for demo purposes it serves its purpose well enough.

**Demo introduction**

So far in this module you've seen triangulation applied on the unit testing level but you're going to also apply triangulation on the system level or in a spike if you will. The simple Web Token I've been developing throughout this module can be used to supply a user name throughout the spike. This demo will show you how I tie all of those things together and get rid of the hard coded user name in favor of a user name supplied through a simple Web Token.

**Demo: using the Simple Web Token through the Spike**

It's been a while since you last saw the http client factor class which is a piece of test infrastructure that sits up in process self hosting for the running journal api. It's always good when you don't have to spend a lot of time with your infrastructure code because that's a good indication that you've created something robust. In any case the http client factoring class is the central place I can modify in order to add an http authorization header to all http requests made in the acceptance tests. In all the scheme for the authorization header should be bearer and the value should be a simple Web Token. You may recall that the current code hard codes the user name FOO so I'll use that as the default user name in order not to break any existing tests. All tests are still passing but that's hardly surprising. I added an http hitter to all requests but since no code consumes this header it doesn't make a lot of difference. As you saw in the previous demos, parameterized tests is a fantastic tool for triangulation. So far I haven't used the theory attribute in the acceptance test project but I'm going to do that now so I add the X unit extension new get package to the project just as I previously did with the unit testing project. Over in the home json tests class I'm now going to convert the existing unit test get root returns correct entry from data base to a program that parameterized test.First I changed the factor attribute to a theory attribute and add and inline data attribute. I wish to parameterize this test on the user name so for the first test case I use the user name already hard coded in the test body. FOO. After adding the user name perimeter I replace the hard coded FOO values with the user name parameter. This is the same test as before so it's hardly surprising that all test still pass. This is only the first step in parameterizing the testbecause the http claim factor still uses the hard coded value FOO internally for the new authorization header I just added. In order to send a parameterized user name form the client to the service I must also enable parameterization of the create method. So I add an overload that takes and user name as an argument. In order not to break existing tests I leave the old create method in place but I can refactor the implementation so that it simply invokes the new overload with the hard coded value FOO. All tests still pass. Now I can use the new overload to pass the username perimeter from the test client to the service. The running journal api now receives that authorization header with the parameterized user name but since the only user name used so far is FOO all tests still pass. The test method is now fully parameterized on the user name so I add two new test cases, BAH and BAS. A single extra test case would have sufficed but I like having at least three test cases for a parameterized test. Not surprisingly when I run all tests the two new tests cases fail. This is what I've been aiming for all along so I can finally go to the journal controller to get rid of the hard coded user name. The try-pass method requires and out perimeter so I first define a simple Web Token variable, then I invoke the try-pass method with a value of the authorization header. This gives me a populated simple Web Token for the potentially many claims so I select the single claim that has the type user name and assign the value to the user name variable. Finally I can replace FOO with a variable. This takes care of the get method but I need to do the same in the post method so I cover the code that reads the user name and paste it into the post method. This enables me to replace the last FOO with the user name variable. Now all tests pass again including the two new ones.

**Demo recap**
To recap this demo show triangulation on a spike. I parameterized an acceptance test and the reason I did that was so that I could use the different test case parameters as part of the authorization http header under the client sends to the service. In the service the journal controller read the user name from the authorization header and used it accordingly.

**Concerns**
You may still be left with a nagging feeling that something's not right yet, didn't I just write some pretty awful code? What about defensive coding? What happens if the try-pass method returns false? That's going to mean that the SWT variable will be null resulting in a null reference exception on the next line.What happens if the authorization header is null? Surely that's going to throw another null reference exception. What happens if there is no user name claim? That's going to throw and invalid operation exception. What if the authorization scheme isn't bearer? That's perhaps the most insidious omissionbecause it might theoretically cause a security breach if the code incorrectly interprets the value of the authorization header with out first checking the scheme? Well, all the tests pass but surely this isn't satisfactory. In the next module you'll see how you can use behavior verification to address such concerns.

**Summary**
In this module you learned about triangulation which is one of the two major approaches to unit testing. It fits best when testing code with a heavy algorithmic component. It also helps if the system under test has few or no external dependencies. Triangulation is a bit like science. The more facts you can gather about a system the greater your confidence in that system will be but ultimately there are no guarantees. You also learned about the Devil's Advocate technique and it's slightly twisted variation Gollum style. It helps you to write the right test code that in test cases table that always move the code forward in a constructive manner. You saw demos on both unit testing level and the system level using the triangulation approach. In the next module you'll learn about triangulation's counterpart, behavior verification.

## Behavior Verification

**Introduction**
Mark Seemann: Hello. My name is Mark Seemann and this is the Outside in Test Driven and Development course Module 4, Behavior Verification. In the last module, you learned about triangulation, which is one of two major approaches to unit testing. In this module, you'll learn about behavior verification, which is the other major approach. You'll also learn about something called characterization tests and sticking with the style from the previous modules in this course there's also going to be extended demos of everything.

**Prerequisites**
Apart from the pre-requisites generally required for this course, in this module I'm going to assume that you know some extra steps. First of all I'm going to touch on the concept of dependency injection but since this isn't a course about dependency injection I'm going to assume that you have at least a general idea about how it works at least in the conceptual level. I'm not going to use any particular library in this course so you don't have to know say Unity Castle Windsor or Ninject to follow along here. Still if you feel that you need to brush up on dependency injection you can view the Pluralsight course on inversion of control. Secondly, I'm going to assume that you are familiar with the concept of dynamic mocks. This was covered in the test first development Part I Pluralsight course. Specifically I'm going to be using the library called mock with a q. Even if you don't know this specific library you should be able to follow along if you understand the general concepts. Still if you'd like to learn about mock, Pluralsight obviously also have a course called mocking with mock.

**Outside-In versus Bottom-Up**

In Module 1, you learned that there are two major approaches to test-driven development; outside in and bottom up. In the previous module, you learned about the bottom up approach. In this module, you'll learn more about the outside in approach. The names London School of TDD Behavior Verification and mockist are more or less synonyms but I prefer behavioral verification because it's the most descriptive.

**Behavior Verification**

Quickly told behavior verification can be used to prove that each unit interacts correctly with its dependencies. This can be used to reduce the number of permeations required to fully cover the functionality of a system under test reducing the number of test cases you'd have to write.

**How many things can go wrong here?**

The last demo in the previous module enabled the client to transmit a user name to the service but the supporting code isn't the most robust code you've ever seen. How many things can go wrong in these three lines of code? If the TryParse method returns false, SWT will be null and the next line of code will grow in our reference exception. If the authorization header is null, another null reference exception will be thrown. If there's no user name claim, the single method will throw an invalid operation exception. If your authorization scheme isn't Bearer, no exception will be thrown but the code might interpret the header incorrectly. Finally, if this request is null, yet another null reference exception will be thrown. What if this request header is null? Well, it turns out that due to the way the http request message class is implemented the header property is never null. No one said that the dot net base class library is consistent.All in all there's 5 ways to fail plus 1 happy path for total of 6 test cases we'll have to write in order to test drive this code block to one or more robust implementation. In the demo code, how many times does this code block appear? It appears in the get method and in the post method so the answer is twice. Keep in mind that getting the user name is a very common thing to do so the only reason it only appears twice is that currently there aren't more controllers in the code base. Wouldn't it help to extract it into a helper method then? Not really because from the internal perspective tests can't verify that this private helper method is being used. All an external test read can see is that the public methods correctly interact with the http request. Tests which still have to test the get method and the post method independently covering all test cases. If they don't, a future developer may change the code so that oneor none of the public methods no longer use the private helper method and yet no tests would fail.

**Triangulate all the things!**
So, if you stay only at the service boundary and try to triangulate the robust implementation you'd have to write not 1 or 2 test cases but 2 times 6 test cases; that's 12 in all just to make the user name code more robust. However, it would be naive to think that only the user name code would need an overhaul. In the post method, the code currently inserts a new user every time a journal entry is being added. So far this has worked because no test cases have tried to add two journal entries for the same user. If you were to try that, an exception would be thrown because you can't add a user with the same user name as an existing user making this code block more robust requires us to check to see if the user already exists and only insert the new row if not.That's 2 test cases instead of just 1. Keep in mind that the user name block is still there requiring 6 test cases to be properly covered.

**Triangulate all the things - not**
To cover the new brands through the code in combination with all the test cases for the user name it's necessary to write 6 new test cases. This brings the total test cases up to 18 and that's for a very small system. For a larger system, the total number of test cases you'd have to write would be enormous if you only write test against the external boundary of the system. This is one of the forces motivating the test pyramid; it's impractical to test only by the external boundary even if that's the most business centric thing to do clearly triangulation isn't the whole solution.

**Cyclomatic Complexity**
The number of test cases you have to write is related to the cyclematic complexity of the solution. Cyclematic complexity is an old and established measurement of software complexity. Essentially time counts the number of paths through a member. The minimum possible cyclematic complexity is 1because there's always going to be 1 path there a member. To measure the complexity of a given block of code start with 1 and add 1 every time you find the key words such as if, else, case, for, for each, do, while, catch. Because cyclematic complexity measures the number of ways through a block of code they also correlate strongly to the number of test cases you'd have to write in order to cover the code.

**Code Coverage**
A quick note on code coverage is in order. Code coverage is a measure of how much of the code of the system on the test is being exercised by giving test read. It's a relative number between 0 and 100%. That number may not mean a lot in itself but it can be interesting to watch the trend. If coverage decreases regularly, you should look into why that is happening. To keep coverage constant the number of test cases should correlate with the cyclematic complexity of the system. As you add more complexity, you should also add more test cases although in TDD you should add the test cases first. Please be aware that code coverage is not a measure of quality and not even a measure of quality of the test code but the trend can be interesting.

**Componentization**
Because of the problem of having to test all possible permitations of a system it can help tremendously to split the system into smaller components.Imagine, for example, that we split the user name code into separate public class called SimpleWebTokenUserNameProjection. As you have learned, it takes 6 test cases to turn that into a robust implementation. The code that says a new journal entry into the database can be extracted into a public class called at journal entry command. You just saw how it would be necessary to check whether or not the user already exists so at least 2 test cases are required to cover that class. The code that reads the existing journal entries from the database can be extracted to a public class called journal entries query. So, far we've only identified a single test case for that; that would be great but now we have all these independent classes and we don't know whether or not the journal controller uses them or if it uses them correctly. In order to be sure of that, we'd have to write a single test of the get method using steps to ensure that data flows correctly through the system. Similarly, we'd have to write a single test of the post-method using steps and mock to verify that an entry is being correctly saved. All in all that's 6 plus 2 plus 1 plus 1 plus 1 for a total of 11 test cases. Eleven is less than 18 and that difference is going to be more pronounced the larger the system becomes.

**Dependency Injection required**
This is where dependency injection or loose coupling is required. In the rest of the module, I'm going to assume that you are familiar with the construction injection pattern and the concept of programming against interfaces. If you want to learn more about dependency injection, you can watch the Pluralsight course inversion of control or you can read my book.

**Formal proofs**
In the previous module, you learned how triangulation is similar to the scientific method. You measure a stimulus response path until you have gained so much confidence that you decide that you're done. Behavioral verification on the other hand offers something more resembling a formal mathematical proof. Steps can prove how data flows and mocks can prove that side effects occurs or that they don't occur. In the demos, you're going to see some examples.

**Data Flow**
Looking at the journal control from the outside we don't have to care about every detail of the implementation. Rather we can concentrate on how data flows. When the get method is involved, it should first get the user name then it should use that user name to get the journal entries and return the entries to the client. If we can prove that this flow happens, we're closer to proving that the overall system works correctly with fewer test cases.

**Side Effects**
Similarly, when methods have side effects such as the post method, we would expect the method to first get the user name and then use that user name to get the posted entry to add the entry to the data store. If we can prove that this happens, we're verifying the behavior of the system unrelated to any specific implementation.

**Observation**
To contrast behavior verification with triangulation, a major difference lies in what is being observed. With behavior verification we have served what happens between the components whereas with triangulation we have served what is externally visible.

**Stimulus/Response**
In the pattern language of external test patterns, we say that with behavior verification the stimulus is often in direct input that is input into the system under test from one of its internal components. It may be that it raised an event or that time returned a value from a query. When the system under test supplies data to its components, we call that indirect output. This form of indirection of stimulus and response is very closely related to the concept of back door manipulation you learned about in Module 2. Triangulation on the other hand is more direct. So, we call the stimulus for it direct input and the response for direct output.

**Initial coverage from Outside-In**
Consider how much coverage we already have components. When you've completed Module 3, only a single path was being exercised through the user name code and the same is true for each other area. Because I strictly follow the TTD rule of only adding code in response to a failing test, the code coverage of the demo code is currently at 100% but it's only exercising a single path through the entire system. Clearly I need to add more tests.

**Number of tests per unit**
When we look at coverage at the unit level, things are worse. Apart from the simple web token class, which is covered by unit tests, all other code is only exercised by the acceptance tests. This means that while the code is being exercised by automatic tests it's not being exercised by any unit tests. On the unit test level, code coverage of journal control and its sub-components is 0. In order to prove that data flows correctly through the system, unit test coverage of the existing code is required.

**Characterization Tests**
To cover existing code with unit tests you can write characterization tests which is a term introduced by Michael Feathers. A characterization test is written after the system under test code. Normally that's a bad sign, but in the context of outside in TDD it's sometimes necessary in order to reduce the number of permeations you'd otherwise have to cover. Your write a seemingly redundant tests in order to save yourself from writing a lot of other tests. A characterization test characterizes the current behavior of the system under test. You could say that it captures a snapshot of its externally visible behavior.

**Demo introduction**
That's a lot of theory to digest. So it's time for a demo. This demo continues the demos from the previous modules but it doesn't quite pick up where the previous demo ended. In between the end of Module 3 and the demo you're about to see, I did some refacturing. Since this isn't a course on refracturing, I didn't want to walk you through every single refracturing step but in the demo I'm going to give you a brief overview of what has changed. If you're interested, the exercise files available for download with this course includes a get repository where you can see all the small green commits I did in order to safely refracture the code. All the way through that process I kept a close eye on the code coverage of the system in order to be sure that I didn't accidently add any new logic while I was refracturing. The coverage stayed steady at 100% throughout. After a quick tour of the refracturing changes, you'll see how to write two characterization tests of the journal controller. These two tests established the overall behavior of and data flow through the journal controller.

**Demo: Refactor review; Characterization Tests**
The journal controller was the main target of the refracturing, but the overall behavior is the same as in Module 3. The get method now gets the username from a helper method and then uses that user name to get the journal entries from a class field of the interface type journal entries query. The post method also gets the user name from the helper method and saves the entry using a class field of the type at journal entry command. The get user name method is currently a private helper method and as you can see it's just as horrible as where we left it in Module 3. Both class fields are injected into journal controller using the constructor injection pattern. The code that actually reads the entries from the database implements the I journal entries query interface but has been moved to a new public class called general entries query. The code is the same as before. The code that saves a new entry to the database implements the I add journal entry command interface but has been moved to a new public class called add journal entry command. Here as you can see I've also added a piece of logic that checks if the user already exists, but I also wrote an acceptance to cover that test case. In order to enable dependency injection for the asp.net web api, I'm replacing the default I http controller activator with this composition route. You can read more about this in my blog post dependency injection and lifetime management with asp.net web api. For good measure I would like to point out that all tests still pass. While the journal control of classes covered by acceptance tests it's not yet covered by unit tests. So I first have to add a class called journal controller tests. In the new class, I add the first characterization test calling it get returns correct results. Now that journal controller uses constructor injection, I must first define the test toggles to inject into it. The queryStub variable is an instance of mock of I journal entries query. The mock of T class is defined by the mock with a q library. The journal controller also requires a command to save an entry, but I don't need to interact with it in this test case so I simply name it command dummy. It's still required in order to satisfy the compiler, but I don't need it so dummy is the correct term. It's an instance of the mock of I add journal entry command class.The set is the journal controller itself injected with the query stop and the command dummy objects. The journal control is request properties being used in order to find the user name so in order to prevent a non-reference exception it must be assigned a value. Here's a little nasty secret of asp.net web api.When unit testing controllers, you must add the configuration to the request properties. This is a dictionary of objects. The key is well known and defined by the http property keys, http configuration key and the value must be an http configuration instance. If you don't do that, the test is going to throw an exception. Finally, the authorization header of the request should have a Bearer scheme and a simple web token containing a claim of the user name type with a value of foo (phonetic). That's a lot of fixtures set up required to unit test a controller and there's quite a bit of accidental complexity. Normally I have ways to deal with that but in a demo I think it's easier to follow along if I just leave it like this. The query stop must be able to return some entries and these are going to be the entries the test will expect. So it's necessary to find them upfront. It's an array of journal entry model instances each with a time, distance and duration value. I write the first instance copy and paste it twice and added the individual values for slight variations. Everything I need to define the data flow is now in place. I use the query stop to define the data flow stating that if get journal entries is called with value foo it returns the expected array. The test can now get a response by invoking get and get the actual model return by reading the responses content as a journal model instance.Finally, to verify that the result is correct according to the specified data flow the expected array is compared with the actual entries. All test pass including the new test I just wrote. That's different from the normal red green refractor cycle where we would expect the new test to fail, but it makes sense because this is a characterization test. Still, I need to make sure that I wrote a correct assertion and not a so-called false negative, which is a test that always passes even if the system on the test is faulty. In order to verify that, I change the implementation to return an empty array and as expected the test now fails in the correct manner. Good. It means I can go back and undo the change I just made. All tests now pass once more. That was the characterization test for the get method, but I also need to write one for the post method. So I add a new test called post inserts entry. Most of the fixture setup code is the same as before.So I copy that path from the previous test and paste it into the new test method. For the post-test case, I don't need the query test double so I rename it to query dummy to more accurately convey the role it plays in this test. I also rename the command test double to command mock because I'm going to use it as a mock. To exercise the system under test, I create a new journal entry model instance and post it. Finally, I can verify that the add journal entry method was invoked with the entry and the user name foo. All the tests pass including the one I just added. Once again I must change the implementation to verifythat I wrote the correct assertion I just comment out the line where it saves to the data store. Now the test fails in the expected manner stating that the mock expected an invocation at least once but that it was never performed. That's good because it means that I wrote the correct test and I can now undo the change that I just made. Now all tests pass once more.

**Demo recap**
In this demo, I first showed you an overview of the refracturing I've done since the demos in Module 3. The most important points is that I've decoupled the journal controller into multiple units using lose coupling. The other thing I changed was that I implemented a check for the existing user in the post methodso that if the user already exists the code doesn't try to insert a new role for that user in the database. After that I wrote a couple of characterization tests of the Get and Post methods. This ensures that the get and post methods are also covered by unit tests as well as the automated acceptance test that we already have.

**Demo introduction**
Now that you have seen how to write a characterization test it's time to see some test driven behavior verification. In the next demo, you'll see how to define data flow and how to specify that side effects should occur. You'll see both of these variations applied to the journal controller class.

**Demo: Data Flow; Side Effects**
Once again before this demo I've implemented a few off-screen changes. The only thing I've done is that I've extracted the private get user name method to a class that implements an interface called I username projection, which is injected into the journal controller along with its other dependencies. The public simple web token projection class implements the interface with the same not very robust code you've now seen quite a few times already. In the journal control a test class, I've also modified the characterization test to reflect that change. The projection stuff is a mock of I username projection and injected into the sut. Notice that this is now a parameterized test with the username as the parameter. The projection stop is configured to return this username.Likewise the test of the post method has undergone the exact same changes. Parameterized over the username the projection stop that is injected into the sut are configured to return the username. The whole point of this exercise is to establish what happens in the journal controller if the get username method can't extract a username from the http request. So, I add a new test called get without user name returns correct response. To deal with the complex fixed setup required for these test cases, I copy the code from the previous test and paste it into the new test. Normally I wouldn't be doing so much copy and pasting, but for demo purposes I do it this way because I think it makes the similarities as well as the differences between the various test cases clearer. This test case defines what happens when the get username method can't project a username from an http request. So it doesn't matter which specific request object it is. To indicate that protection wasn't possible, I configure the stop to return null. Here I need to stop and give a warning. Normally I consider returning null at poor design decision but throwing in an exception isn't a good solution either. In cases like this, I'd much rather than a cue from functional programming and return an option type or a maybe monad, but since this isn't a course on multi-parametric design, for educational purposes I use null as a compromise. The response is the result of invoking the get method and I expect the response to have a status code of unauthorized which I compare with the actual status code. The test fails in the expected manner; the actual status code is okay; not unauthorized. The failing test prompts me to check if the username is null and return an error response of unauthorized if that's the case. All tests now pass. That takes care of the get method but I also need to do with the same with the post-method. So I add a new test called post without user name returns correct response. As before, I covered the fixture setup from a previous test but take care to rename command mock to command dummy since this test double isn't being used in this test case. Once again I configure the get user name method to return null. A dummy entry is required in order to invoke the post method. As before, the test expects that the http response is unauthorized which it compares with the actual response from the sut. This test also fails as expected so I go to the post method to make it pass. The scenario is the same as for the get method so I copy the code from there and paste it into the post method. All tests now pass. That's good but just because we're now doing behavior verification we shouldn't forget about the Devil's Advocate technique. Could I change this implementation and still pass all tests?What if I take this username check and move it to after the add journal entry call? All tests still pass. That can't be right because that's going to mean that the add journal entry method can be involved with a null username. That's very likely to throw an exception if the data store can't handle that but even if no exception is thrown, it would be incorrect because that would mean that the entry was actually saved but the client was told that the request failed with an unauthorized status code. It turns out I need the command test double as a mock after all. Using it I want to verify that the add journal entry method is never invoked no matter which journal entry was supplied and no matter the value of the username. To indicate that it should never be invoked, I passed times never. The test now fails because of the mock that states that it expected that an invocation of the mock should never have been performed but was performed at one time. So, in the post method, I move the add journal entry call to a position after the null check. All tests now pass.

**Demo recap**
In this demo, you saw how to specify data flow. When we combine the language of the command query separation principle with the xunit test patterns language, we can say that specifying data flow is relevant for queries such as the get method and we use stops for the specification. You also saw how I specified side effects using mocks. This makes sense because the post method is a command. Finally, you saw that the devil's advocate technique also makes sense for behavior verification and that we may need to apply a mock to verify that under certain conditions some events don't occur.

**Demo introduction**
Now that we know that the journal controller interacts correctly with the username protection, we can finally turn our attention to the get username method in order to make it more robust.

**Demo: making the user name code robust**
Now that we have established the behavior of the journal controller class it's time to turn our attention to the simplewebtokenusername projection class. So I add a new unit test class called simple web token username projection tests. There's no unit test coverage of the simple web token username projection class so the first test must be yet another characterization test. I call it get username from proper simple web token returns correct result. The sut is obviously a new instance of the simple web token username projection class. I also need a request variable as a new http request message instance. To continue simple web token its authorization header must have a scheme of bearer and a value of simple web token with a claim with the username type and foo as a value. The actual value is the result of invoke and get username with the request. Finally, the test asserts that the expected value foo is equal to the actual value. All tests pass. As this is a characterization test, I need to see it fail in order to verify that I wrote the correct assertion. You may find this just a tad compulsive, but you'd be surprised how easy it is to inadvertently add a false/negative. These can be quite subtle and difficult to spot. So I choose to err on the side of caution. At least once a week it prevents me from doing something stupid. In the sut, I hard code bar as a return value and this causes the new test to fail as expected stating that foo was expected but bar was returned. To fix this I undo my change and all tests now pass. Playing devil's advocate on the other hand I realize that if I change the return value to foo, all tests still pass. As you learned in Module 3, the best way to address this is to convert the test to a parameterized test. So, I change the fact attribute into a theory attribute and add an in-line data attribute with a value foo. This is the expected value and in the test method's body I replace all instances of foo with the expected parameter. This is the same test case as before just expressed in a different way. All tests still pass. Now it's easy to add two new test cases -- bar and baz -- which fail as expected. This prompts me to go back into the sut and undo my change. All tests now pass. This fully characterizes the current behavior of the simple web token username projection class. So now it's time to add test cases for all the failure conditions. I start with the simple test case and get username from null request throws. The sut is a new simple web token username projection instance and I assert that it throws an argument null exception when I invoke the get username method with a null request. The test throws a non-reference exception at this line in the sut. This doesn't count as a properly failing test so I must first put the sut into a state where I see that the assertion is being exercised. I add a null guard and return the empty string and run the test again. This time the assertion is exercised stating that it expected an argument null exception but that no exception was thrown. To past that test, I threw an argument null exception from the get user name method. All tests now pass. The next test case is get username from request without authorization header returns correct result. The sut is a new simple web token username protection instance. I also need an http request message instance but now I do something unusual. I add an assertion in the fixture setup face of the test. This is called a got assertion and the reasoning goes like this. The condition for this test case is that the authorization header should be null. The default value for the authorization property is, indeed, null, but I can't be sure that this is always going to be the case. The http request message class doesn't belong to me; it belongs to Microsoft and I don't think it would be a break in change if they decide to change the default in the future. Perhaps they would decide to apply the null object pattern; I know I would. However, since it's an essential part of this test case that the authorization property is null I added this null assertion here. It's not the actual test, but if the http request message defaults, if a change after a service pack, I'll get a correct test failure right there and I'll know that there's nothing wrong with my code but that my assumptions about the default authorization property no longer holds.Anyway, now I can involve the get username with the request and assert that the actual value is null. The test fails with a null reference exception at this line of sut code. I add an if statement that checks whether the authorization property is null and returns foo if it is. This enables me to see that I wrote the correct assertion because it now fails correctly stating that an expected null but that the actual value was foo. Now I can change the return values to null. All tests now pass. The next test case is get user name from request with incorrect authorization scheme returns correct result. The sut is a new simple web token username protection instance and the request is a new http request message instance. This time I assign a value to the authorization property with the scheme value of invalid. Since this is the incorrect input I should keep everything else in the test correct so the authorization value is going to be a simple web token with a claim with a type of username and a value of dummy. Invoking get username with this request I assert that the actual value is null.However, running the tests shows that the actual value is instead dummy; the username value from the simple web token. Now I'm playing devil's advocate again and implement the scheme check exactly as the test case specifies. I check if the scheme is invalid and return null if it is. This passes all tests. By now you should know what that triggers; a parameterized test. I change the fact attribute to a theory attribute and add an inline attribute with the string invalid.In the test method body, I replace the hard-coded string with the invalid scheme test parameter. This is the same test as before and it still passes. Knowing my own gollum tendencies I add new test cases making sure to prevent myself from doing another round of devil's advocate by doing an ends with contains or starts with comparison. As expected, the three new test cases fail so I change the get username method to correctly check for the bearer string.All tests now pass. The next test case is get username from invalid symbol web token returns correct result. The sut is a new simple web token username protection instance and the request a new http request message instance. This time the authorization header gets the correct scheme Bearer but an invalid token value invoking the get user name method the test asserts that the actual value is null. However, when running the test, a null reference exception is thrown at this line of sut code because the S-W-T variable is null. In order to see the test fail correctly, I check the return value from the try pass method and return foo if it's false. Now the test fails as expected stating that it expected null but that the actual value was foo. This causes me to return null instead of foo. All tests now pass. The final test case is get username from simple web token with no username claim returns correct result. The sut is a new simple web token username protection instance and they request a new http request message instance. This time the authorization header gets the correct scheme bearer and correct simple web token with a claim with the type some claim and the value dummy but without a username claim. Invoking the get username method, the test asserts that the actual value is null; however, when running the test, an invalid operation exception is thrown at this line of sut code because the single method doesn't find a username claim. In order to see the test fail correctly, I check to see if there are any username claims in the simple web token and return foo if there's none. Now, the test fails as expected stating that an expected null but that the actual value was foo. This causes me to return null instead of foo. All tests now pass. We shouldn't forget about the red green refractor cycle. I've been doing a lot of red and green but refactoring is long overdue. The last few statements can be turned into a link query so that it includes only the claims where the type is username then selects the claims value and returns the single matching value or the default which is null. All tests still pass and now the get username method looks much more robust.

**Demo recap**

In this demo, you saw how behavioral verification enabled me to isolate the simple web token username protection class so that I could test it properly 'ceteris paribus' or all other things equal, but did you notice something? I used triangulation on the simple web token username protection class. This is quite normal. The behavior verification technique is more impure than pure triangulation because it mainly concerns itself with verifying that the interaction between units is correct. What each unit does can be tested with more behavioral verification or with triangulation.

**Monolith**

Throughout this course you've seen me build a single monolithic demo and you may be wondering is this now supposed to be good? What happened to layout architectures? Doesn't this spit in the face of years of best practices? Well, the code base I've created is actually very loosely coupled. This tends to be a side effect of developing code with test-driven development technique. There's nothing preventing me from now decoupling my monolithic code into multiple libraries; however, this isn't a course I'm building loosely coupled systems, but if you want to know more about that you can always read my book.

**Summary**

In this module, you learned about behavior verification which is one of two major approaches to unit testing. Using steps it enables you to define how data flows through a unit. Using mocks it enables you to measure whether or not side effects happened. While triangulation reminds me of the scientific method,behavior verification is a bit more like math. It's almost as if from established pre-requisites it could be proven that a given unit always behaves in a certain way. Sometimes when starting at the boundary of a system and working our way in, we may leave some classes uncovered by unit tests although they are covered by acceptance tests. In order to establish the behavior of such units, we may sometimes have to write characterization tests to capture the current behavior of a unit. This enables us to properly isolate each unit independently of each other but still verify that they interact correctly with each other. This, again, helps us to reduce the number of tests we have to write. This is the final module of the outside in test driven development course. My name is Mark Seemann, and I hope that you found it as exciting to watch the course as making it was for me.
