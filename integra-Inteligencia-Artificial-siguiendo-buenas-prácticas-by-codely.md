# Integra Inteligencia Artificial siguiendo buenas pr√°cticas (OpenAI GPT, Ollama y LangChain)

## General

- <https://pro.codely.com/library/integra-inteligencia-artificial-siguiendo-buenas-practicas-openai-gpt-ollama-y-langchain-222393/598318/path/>
- <https://github.com/CodelyTV/add_ai_follwing_best_practices-course>
- Duraci√≥n estimada: ~ 4 hours

## üöÄ ¬øC√≥mo empezar con la Inteligencia Artificial como Developer?

- [ChatGPT Prompting vs RAG vs fine tuning: Aprovecha la IA al m√°ximo](https://www.youtube.com/watch?v=bjCdsnkQ6Dw), video, 69 minutes
  - 13.02.2024
  - 3 ejemplos para diferenciar lo que podemos conseguir con prompt engineering, vs. RAG (Retrieval-Augmented Generation) vs. fine tuning de modelos de Inteligencia Artificial.
- [C√≥mo sacarle el m√°ximo provecho a ChatGPT | #laFunci√≥n8x28](https://www.youtube.com/watch?v=sYZHBO3HRhA), video, 76 minutes
  - 25.04.2023
  - La IA es una herramienta muy poderosa que podemos utilizar en nuestro d√≠a a d√≠a. En el directo de hoy Bea Mart√≠n Valc√°rcel (@zigiella) nos ayudar√° a explorar diversas formas de darle utilidad.

### Conceptos de integraci√≥n con IA

- **Modelos**
  - GPT (ChatGPT es la aplicaci√≥n para el modelo GPT)
  - LLaMA, liberado por Meta, Open Source
  - Gemma (de Google, pesa poco, mejor para ordenadores no tan potentes)
  - Command R, open source, requieres m√≠nimo 64 GB RAM local.
  - [Mistral](https://mistral.ai/)
  - Suno (audio)
  - Stable Diffusion (im√°genes)
  - Lo que var√≠a entre ellos es **c√≥mo se interacciona con ellos**
- **Inferidor/servidor**
  - LLM server
  - C√≥mo se expone el modelo
  - [LlaMAcpp](https://github.com/ggml-org/llama.cpp) es el m√°s conocido, genera un binario.
    - Sirve para varios modelos, no s√≥lo para LLaMA.
  - Quiero desplegarlo, normalmente necesitas varios servidores. Muy caro en la nube.
  - **OLLaMa** es la m√°s famosa abstracci√≥n sobre LLaMA hoy en d√≠a. Por debajo usa LLaMAcpp.
    - Ofrece un API HTTP
- **SDKs**
  - Abstracci√≥n que facilita la integraci√≥n
  - **LangChain**: para JS y Python (ciudadano de primer nivel), el m√°s famoso
    - **LangChain4j**: para JVM
    - Permitir aplicar t√©cnicas de RAG, donde se complementa la informaci√≥n del modelo con tu propia "base de datos".
    - Puede interactuar con las APIs de OLLaMA y similar, pero tambi√©n con los modelos directamente.
  - **Resonance**: para PHP
  - **XEF.ai**: para Kotlin
  - **ModelFusion**: para JS/TS, tambi√©n recomendado por LLaMAcpp
  - [microsoft/semantic-kernel](https://github.com/microsoft/semantic-kernel): Integrate cutting-edge LLM technology quickly and easily into your apps
    - [Introducing new Ollama Connector for Local Models](https://devblogs.microsoft.com/semantic-kernel/introducing-new-ollama-connector-for-local-models/)

### Primeros pasos con OLLaMA

- [C√≥digo fuente](https://github.com/CodelyTV/add_ai_follwing_best_practices-course/tree/main/01-intro/2-ollama)
- <https://ollama.com/>
  - `brew install ollama`
  - `ollama serve`: tambi√©n podr√≠as tenerlo en segundo plano todo el tiempo, con `brew`
  - Descargar un modelo, e.g. `ollama pull gemma:latest`
    - Similar a Docker. Se puede dockerizar, pero en Mac no hay virtualizaci√≥n de la GPU e ir√≠a lent√≠simo.
  - `ollama run gemma:latest`, para arrancar el modelo.
  - ¬øQu√© modelo utilizar?
    - Prueba y error
    - Depende del tama√±o de RAM que tengas
- Por defecto corre en el puerto 11434 (regla mnemot√©cnica: el n√∫mero parece LLAMA)
  - See server status: `curl http://localhost:11434 -v`
  - List all the models downloaded: `curl http://localhost:11434/api/tags | jq`
  - Download a model: `curl --request POST http://localhost:11434/api/pull -d '{"model": "gemma:latest"}'`
    - Similar to `ollama pull gemma:latest`
  - Ask a model: `curl --request POST http://localhost:11434/api/generate --header "Content-Type: application/json" -d '{"model": "gemma:latest", "prompt": "Qu√© es CodelyTV? (en menos de 15 palabras)", "stream": false}' | jq`
    - El `stream: false` devuelve toda la respuesta de golpe, para evitar enviar poco a poco la respuesta (puede interesar o no, depende).
    - La respuesta incluye "context", que habr√≠a que volver a enviar si hici√©ramos una nueva consulta relacionada con √©sta, para que la considerase de la misma conversaci√≥n.
      - El uso de SDKs facilita esto, no tienes que encargarte de ello a mano.

### Desmitifica apps con IA: LangChain vs ModelFusion

- [C√≥digo fuente](https://github.com/CodelyTV/add_ai_follwing_best_practices-course/tree/main/01-intro/3-integrate_model)
  - `curl http://localhost:3000/api/langchain?prompt=Que%20es%20CodelyTV`
  - `curl http://localhost:3000/api/modelfusion?prompt=Que%20es%20CodelyTV`
- Alternativas:
  - <https://js.langchain.com/docs/introduction/>
    - Tambi√©n existe para Python
    - LangChain is a framework for developing applications powered by large language models (LLMs).
  - <https://github.com/vercel/modelfusion>
- LangChain tiene mucha m√°s comunidad...

## üí° Arquitectura de software: C√≥mo encaja la IA en tu app

- **Caso de uso**: ofrecer sugerencias de cursos a un usuario que acaba de terminar un curso.
- [C√≥digo fuente](https://github.com/CodelyTV/add_ai_follwing_best_practices-course/tree/main/02-software_architecture/2-implementation_details)
- Onion Architecture:
  - Infrastructure
    - OllamaMistralUserCourseSuggestionsRepository
  - Application
  - Domain
    - UserCourseSuggestionsRepository

## üë§ Caso pr√°ctico: Perfil de usuario con sugerencias de cursos

- **Alternativa 1: Orquestado por caso de uso**
  - [C√≥digo fuente](https://github.com/CodelyTV/add_ai_follwing_best_practices-course/tree/main/03-user_profile_wth_suggestions/1-llm_use_case)
  - En el caso de uso "UserFinder" inyectamos un UserRepository y un CoursesSuggestionsRepository
  - No les convence porque tienen que actualizar el agregado User con las recomendaciones, a posteriori
- **Alternativa 2: Encapsulado en UsersRepository**
  - [C√≥digo fuente](https://github.com/CodelyTV/add_ai_follwing_best_practices-course/tree/main/03-user_profile_wth_suggestions/2-llm_in_repo)
  - Apuestan por esta opci√≥n
  - En el UserFinder inyectamos un UserRepository, quien a su vez tiene una dependencia/inyecci√≥n del UserCourseSuggestionsRepository
  - En realidad deber√≠amos haber de un LoggedInUserRepository, ya que no querr√≠amos ver los cursos recomendados para un usuario cuyo perfil estuvi√©ramos visitando (y que tambi√©n hubiera hecho una llamada al mismo Repository)

## ‚å®Ô∏è Caso pr√°ctico: Prompting con Ollama

- [C√≥digo fuente](https://github.com/CodelyTV/add_ai_follwing_best_practices-course/tree/main/04-llm_implementation_details/1-implement_a_recommendator)
  - Hints para el prompt:
    - Usar "IMPORTANTE", le da importancia a las may√∫sculas.
    - A veces necesitas repetir las instrucciones, tanto en positivo como en negativo.
    - Usar "system prompt" para aquello que deba mantenerse a lo largo de una conversaci√≥n.
  - Ollama cachea por defecto si el prompt es el mismo.
- **Determinismo v√≠a cach√©**
  - [C√≥digo fuente](https://github.com/CodelyTV/add_ai_follwing_best_practices-course/tree/main/04-llm_implementation_details/2-cache)  
  - Mejora en rendimiento y en el coste computacional.
  - La cach√© no es la soluci√≥n ideal, pero ya es una mejora.
  - Esta cach√© en DB podr√≠a ser mejorada almacen√°ndolo en un Redis con TTL o a√±adiendo un campo de timestamp para las recomendaciones.
  - Ejemplo de wrapper para tener cach√© in memory: [InMemoryCacheUserRepository](https://github.com/CodelyTV/add_ai_follwing_best_practices-course/blob/0b435d4c031fa2ed986cdfeb8b3f1459e14ae8eb/04-llm_implementation_details/2-cache/src/contexts/mooc/users/infrastructure/InMemoryCacheUserRepository.ts#L5)

## ‚úçÔ∏è Traslada el coste computacional al momento de escritura

[Code examples: LLM with domain events](https://github.com/CodelyTV/add_ai_follwing_best_practices-course/blob/main/05-event_driven/2-llm_with_domain_events/)

- [InMemoryEventBus](https://github.com/CodelyTV/add_ai_follwing_best_practices-course/blob/95fd558ae5ec9d98064d492210b993ec19fdc74d/05-event_driven/2-llm_with_domain_events/src/contexts/shared/infrastructure/domain_event/InMemoryEventBus.ts#L9-L9)

![](ai-best-practices-codelytv/ports_and_adapters_with_llm.png)

- Queremos quitarnos la dependencia de Ollama en tiempo de b√∫squeda, para evitar tiempos de espera adicionales, mayor coste, etc.
- A√±adiremos un campo "suggested_courses" de tipo TEXT en la tabla `users`

![](ai-best-practices-codelytv/flujo_peticion_completar_curso.png)

- `UserCourseProgressCompletedDomainEvent`
![](ai-best-practices-codelytv/flujo_peticion_generacion_sugerencias_1.png)
![](ai-best-practices-codelytv/flujo_peticion_generacion_sugerencias_2.png)
- `UserCourseSuggestionsGenerated`
![](ai-best-practices-codelytv/flujo_proyectar_sugerencias.png)
- More simple solution: just a join between both tables (the table with the user suggestions and the user table)

![](ai-best-practices-codelytv/flujo_total_completar_curso.png)

## üó£Ô∏è C√≥mo integrar tu app con OpenAI GPT

- [C√≥digo de ejemplo](https://github.com/CodelyTV/add_ai_follwing_best_practices-course/tree/main/06-chatgpt/1-add_chatgpt)
- Seguimos usando LangChain como SDK, pero ahora con el modelo GPT a trav√©s de su inferidor/servidor HTTP.
- De pago, hay que crear una clave con las capacidades necesarias.
- **Qu√© es un token y c√≥mo se calculan** (y relaci√≥n con el precio)
  - [C√≥digo: tokens calculation](https://github.com/CodelyTV/add_ai_follwing_best_practices-course/tree/main/06-chatgpt/2-tokens_calculation)
  - Qu√© influye en el precio de GPT
    - Cantidad tokens de input
    - Cantidad tokens de output
    - Los tokens m√°ximos permitidos es la suma de los de input y los de output
  - Un token est√° entre una s√≠laba y una palabra
  - Concepto de "ventana de contexto"
  - <https://platform.openai.com/tokenizer>
    - En m√∫ltiples modelos:
      - <https://www.prompttokencounter.com/>
      - <https://tiktokenizer.vercel.app/>
      - <https://gpt-tokenizer.dev/>
  - A helpful rule of thumb is that one token generally corresponds to ~4 characters of text for common English text. This translates to roughly ¬æ of a word (so 100 tokens ~= 75 words).
  - Se pueden a√±adir callbacks a la llamada a OpenAI, e.g. `handleLLMStart`y `handleLLMEnd`
- **üí∏ Precios: Estimaci√≥n de costes de la API de OpenAI**
  - Versi√≥n "Turbo": pasado un tiempo de una versi√≥n, sacan la Turbo, que es m√°s potente y ya suele costar menos.
  - Modelo de expansi√≥n, e.g. "GPT-4-32k", con 32k siendo los tokens m√°ximos en la conversaci√≥n
  - GPT-3.5 Turbo
  - El precio se calcula por cada 1.000 tokens, e.g. para input:
    - GPT-3.5 Turbo:  0,0005 $
    - GPT-4:          0,03 $
    - GPT-4-32k:      0,06 $
    - GPT-4 Turbo:    0,01 $
  - Los precios son diferentes para input y para output (para output suele ser el doble)
  - <https://openai.com/api/pricing/>
  - <https://platform.openai.com/docs/pricing>
  - Gracias a los eventos y el Event Bus, podemos controlar la velocidad de consumo de los eventos para no superar el l√≠mite de peticiones por minuto en OpenAI

## üèÅ Buenas pr√°cticas aplicadas en la integraci√≥n con LLM

### üìñ T√©cnicas de Prompting: Zero-shot vs Few-shot vs Chain-of-Thought

- **Zero-shot**
  - Hacemos un prompt indicando lo que queremos.
  - Se suele iterar el prompt: volvemos al "prompt inicial" para ir refin√°ndolo.
  - <https://www.promptingguide.ai/techniques/zeroshot>
- **Few-shot**
  - Damos varios ejemplos para que contin√∫e... Cada ejemplo es un "shot".
  - Acabamos con `-` para que contin√∫e con su propio ejemplo generado
  - <https://www.promptingguide.ai/techniques/fewshot>
- **Chain-of-Thought (CoT)**
  - Le explicamos la l√≥gica de lo que esperamos, de los ejemplos.
  - Le obligamos a "razonar".
  - Acabamos con `>`
  - <https://www.promptingguide.ai/techniques/cot>

### üî´ Implementa Few-Shot Prompting con LangChain

- [C√≥digo de ejemplo: implementaci√≥n de "few-shot"](https://github.com/CodelyTV/add_ai_follwing_best_practices-course/tree/main/07-good_practices/2-implement_few_shot)
- El `prefix` es el inicio del prompt.
- Para los ejemplos se puede usar un `PromptTemplate`
- Indicas la m√°xima longitud que quieres para los ejemplos, el m√°ximo n√∫mero de caracteres: `maxLength`

### üßæ Consigue respuestas m√°s robustas: Tipado con JSON Schema

- [Ejemplo de c√≥digo para a√±adir tipos](https://github.com/CodelyTV/add_ai_follwing_best_practices-course/tree/main/07-good_practices/3-add_types)
- <https://zod.dev/>: permite tipar. Se podr√≠a hacer con TS, pero los tipos en TS desaparecen al ser transpilados a JS. Por otra parte, permite convertirlo en un JSon Schema. Bastante usada, e.g. para las requests de HTTP que llegan o formularios web.
- Evitas tener que parsear la respuesta. Queremos tener una respuesta tipada por parte del LLM.
- [Ejemplo parseardor con Zod](https://github.com/CodelyTV/add_ai_follwing_best_practices-course/blob/95fd558ae5ec9d98064d492210b993ec19fdc74d/07-good_practices/3-add_types/src/contexts/mooc/user_course_suggestions/infrastructure/OllamaMistralCourseSuggestionsGenerator.ts#L34-L34)

## ‚úÖ Testea la integraci√≥n con tu LLM

### üêõ Feedback loop m√°s r√°pido para depurar tu prompt
- [Ejemplo de c√≥digo: debug prompt](https://github.com/CodelyTV/add_ai_follwing_best_practices-course/tree/main/08-testing/1-debug_prompt)
- C√≥mo iterar los prompts r√°pidamente.
  1. `ollama run mistral`
  2. Copiar el prompt y ver qu√© devuelve
  3. Modificar lo necesario y volver al punto 1

### üß™ C√≥mo hacer tests de integraci√≥n a tu LLM
- [C√≥digo con tests de integraci√≥n del LLM](https://github.com/CodelyTV/add_ai_follwing_best_practices-course/tree/main/08-testing/2-integration_tests)
- [OllamaMistralCourseSuggestionsGenerator.test.ts](https://github.com/CodelyTV/add_ai_follwing_best_practices-course/blob/95fd558ae5ec9d98064d492210b993ec19fdc74d/08-testing/2-integration_tests/tests/contexts/mooc/user_course_suggestions/infrastructure/OllamaMistralCourseSuggestionsGenerator.test.ts#L1-L1)
- Se basa en el evaluator de LangChain... pero usa un prompt diferente en el c√≥digo de test respecto a producci√≥n (comentan que por el JSon Schema), pero IMO eso invalida la confianza y relevancia de los tests :-/

### ‚ôªÔ∏è Configura tu entorno de CI para Ollama y GPT
- [Ejemplo de c√≥digo con CI](https://github.com/CodelyTV/add_ai_follwing_best_practices-course/tree/
main/08-testing/3-llm_ci)
- Para ejecutar `OpenAIChatGPT35CourseSuggestionsGenerator` s√≥lo en la pipeline, usa nomenclatura `.ci.test` y filtra en los npm scripts (`test` vs `test:ci`)

## üîú Conclusiones y siguientes pasos

- M√°s cursos
  - [IA: Embeddings y RAG](https://pro.codely.com/library/ia-buscador-con-datos-propios-usando-rag-230838/655241/about/)
  - [Embeddings autom√°ticos en Postgres](https://pro.codely.com/library/embeddings-automaticos-en-postgres-236271/702554/about/)

## More links

- <https://www.youtube.com/playlist?list=PLZVwXPbHD1KMRRUApy-cVI7q6B5QhoF3H>
- [Deploy ANY Open-Source LLM with Ollama on an AWS EC2 + GPU in 10 Min (Llama-3.1, Gemma-2 etc.)](https://www.youtube.com/watch?v=SAhUc9ywIiw)
  - <https://github.com/developersdigest/aws-ec2-cuda-ollama>

## Feedback

- [02-software_architecture/2-implementation-details](https://github.com/CodelyTV/add_ai_follwing_best_practices-course/tree/main/02-software_architecture/2-implementation_details): no compila, faltan clases. Tambi√©n incluir info Makefile que facilite arrancarlo todo (e.g. que incluya un "docker compose up" para la DB, etc.)
- [04-llm_implementation_details/1-implement_a_recommendator](https://github.com/CodelyTV/add_ai_follwing_best_practices-course/tree/main/04-llm_implementation_details/1-implement_a_recommendator): idem, no compila por motivos m√∫ltiples.
