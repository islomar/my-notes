# Integra Inteligencia Artificial siguiendo buenas pr√°cticas (OpenAI GPT, Ollama y LangChain)

## General

- <https://pro.codely.com/library/integra-inteligencia-artificial-siguiendo-buenas-practicas-openai-gpt-ollama-y-langchain-222393/598318/path/>
- <https://github.com/CodelyTV/add_ai_follwing_best_practices-course>
- Duraci√≥n estimada: ~ 4 hours

## üöÄ ¬øC√≥mo empezar con la Inteligencia Artificial como Developer?

- [ChatGPT Prompting vs RAG vs fine tuning: Aprovecha la IA al m√°ximo](https://www.youtube.com/watch?v=bjCdsnkQ6Dw), video, 69 minutes
  - 13.02.2024
  - 3 ejemplos para diferenciar lo que podemos conseguir con prompt engineering, vs. RAG (Retrieval-Augmented Generation) vs. fine tuning de modelos de Inteligencia Artificial.
- [C√≥mo sacarle el m√°ximo provecho a ChatGPT | #laFunci√≥n8x28](https://www.youtube.com/watch?v=sYZHBO3HRhA), video, 76 minutes
  - 25.04.2023
  - La IA es una herramienta muy poderosa que podemos utilizar en nuestro d√≠a a d√≠a. En el directo de hoy Bea Mart√≠n Valc√°rcel (@zigiella) nos ayudar√° a explorar diversas formas de darle utilidad.

### Conceptos de integraci√≥n con IA

- **Modelos**
  - GPT (ChatGPT es la aplicaci√≥n para el modelo GPT)
  - LLaMA, liberado por Meta, Open Source
  - Gemma (de Google, pesa poco, mejor para ordenadores no tan potentes)
  - Command R, open source, requieres m√≠nimo 64 GB RAM local.
  - [Mistral](https://mistral.ai/)
  - Suno (audio)
  - Stable Diffusion (im√°genes)
  - Lo que var√≠a entre ellos es **c√≥mo se interacciona con ellos**
- **Inferidor/servidor**
  - LLM server
  - C√≥mo se expone el modelo
  - [LlaMAcpp](https://github.com/ggml-org/llama.cpp) es el m√°s conocido, genera un binario.
    - Sirve para varios modelos, no s√≥lo para LLaMA.
  - Quiero desplegarlo, normalmente necesitas varios servidores. Muy caro en la nube.
  - **OLLaMa** es la m√°s famosa abstracci√≥n sobre OLLaMA hoy en d√≠a. Por debajo usa LLaMAcpp.
    - Ofrece un API HTTP
- **SDKs**
  - Abstracci√≥n que facilita la integraci√≥n
  - **LangChain**: para JS y Python (ciudadano de primer nivel), el m√°s famoso
    - **LangChain4j**: para JVM
    - Permitir aplicar t√©cnicas de RAG, donde se complementa la informaci√≥n del modelo con tu propia "base de datos".
    - Puede interactuar con las APIs de OLLaMA y similar, pero tambi√©n con los modelos directamente.
  - **Resonance**: para PHP
  - **XEF.ai**: para Kotlin
  - **ModelFusion**: para JS/TS, tambi√©n recomendado por LLaMAcpp
  - [microsoft/semantic-kernel](https://github.com/microsoft/semantic-kernel): Integrate cutting-edge LLM technology quickly and easily into your apps
    - [Introducing new Ollama Connector for Local Models](https://devblogs.microsoft.com/semantic-kernel/introducing-new-ollama-connector-for-local-models/)

### Primeros pasos con OLLaMA

- [C√≥digo fuente](https://github.com/CodelyTV/add_ai_follwing_best_practices-course/tree/main/01-intro/2-ollama)
- <https://ollama.com/>
  - `brew install ollama`
  - `ollama serve`: tambi√©n podr√≠as tenerlo en segundo plano todo el tiempo, con `brew`
  - Descargar un modelo, e.g. `ollama pull gemma:latest`
    - Similar a Docker. Se puede dockerizar, pero en Mac no hay virtualizaci√≥n de la GPU e ir√≠a lent√≠simo.
  - `ollama run gemma:latest`, para arrancar el modelo.
  - ¬øQu√© modelo utilizar?
    - Prueba y error
    - Depende del tama√±o de RAM que tengas
- Por defecto corre en el puerto 11434 (regla mnemot√©cnica: el n√∫mero parece LLAMA)
  - See server status: `curl http://localhost:11434 -v`
  - List all the models downloaded: `curl http://localhost:11434/api/tags | jq`
  - Download a model: `curl --request POST http://localhost:11434/api/pull -d '{"model": "gemma:latest"}'`
    - Similar to `ollama pull gemma:latest`
  - Ask a model: `curl --request POST http://localhost:11434/api/generate --header "Content-Type: application/json" -d '{"model": "gemma:latest", "prompt": "Qu√© es CodelyTV? (en menos de 15 palabras)", "stream": false}' | jq`
    - El `stream: false` devuelve toda la respuesta de golpe, para evitar enviar poco a poco la respuesta (puede interesar o no, depende).
    - La respuesta incluye "context", que habr√≠a que volver a enviar si hici√©ramos una nueva consulta relacionada con √©sta, para que la considerase de la misma conversaci√≥n.
      - El uso de SDKs facilita esto, no tienes que encargarte de ello a mano.

### Desmitifica apps con IA: LangChain vs ModelFusion

- [C√≥digo fuente](https://github.com/CodelyTV/add_ai_follwing_best_practices-course/tree/main/01-intro/3-integrate_model)
  - `curl http://localhost:3000/api/langchain?prompt=Que%20es%20CodelyTV`
  - `curl http://localhost:3000/api/modelfusion?prompt=Que%20es%20CodelyTV`
- Alternativas:
  - <https://js.langchain.com/docs/introduction/>
    - Tambi√©n existe para Python
    - LangChain is a framework for developing applications powered by large language models (LLMs).
  - <https://github.com/vercel/modelfusion>
- LangChain tiene mucha m√°s comunidad...

## üí° Arquitectura de software: C√≥mo encaja la IA en tu app

- **Caso de uso**: ofrecer sugerencias de cursos a un usuario que acaba de terminar un curso.
- [C√≥digo fuente](https://github.com/CodelyTV/add_ai_follwing_best_practices-course/tree/main/02-software_architecture/2-implementation_details)
- Onion Architecture:
  - Infrastructure
    - OllamaMistralUserCourseSuggestionsRepository
  - Application
  - Domain
    - UserCourseSuggestionsRepository

## üë§ Caso pr√°ctico: Perfil de usuario con sugerencias de cursos

- **Alternativa 1: Orquestado por caso de uso**
  - [C√≥digo fuente](https://github.com/CodelyTV/add_ai_follwing_best_practices-course/tree/main/03-user_profile_wth_suggestions/1-llm_use_case)
  - En el caso de uso "UserFinder" inyectamos un UserRepository y un CoursesSuggestionsRepository
  - No les convence porque tienen que actualizar el agregado User con las recomendaciones, a posteriori
- **Alternativa 2: Encapsulado en UsersRepository**
  - [C√≥digo fuente](https://github.com/CodelyTV/add_ai_follwing_best_practices-course/tree/main/03-user_profile_wth_suggestions/2-llm_in_repo)
  - Apuestan por esta opci√≥n
  - En el UserFinder inyectamos un UserRepository, quien a su vez tiene una dependencia/inyecci√≥n del UserCourseSuggestionsRepository
  - En realidad deber√≠amos haber de un LoggedInUserRepository, ya que no querr√≠amos ver los cursos recomendados para un usuario cuyo perfil estuvi√©ramos visitando (y que tambi√©n hubiera hecho una llamada al mismo Repository)

## ‚å®Ô∏è Caso pr√°ctico: Prompting con Ollama

- [C√≥digo fuente](https://github.com/CodelyTV/add_ai_follwing_best_practices-course/tree/main/04-llm_implementation_details/1-implement_a_recommendator)

## ‚úçÔ∏è Traslada el coste computacional al momento de escritura

## üó£Ô∏è C√≥mo integrar tu app con OpenAI GPT

- TBD

## üèÅ Buenas pr√°cticas aplicadas en la integraci√≥n con LLM

- TBD

## ‚úÖ Testea la integraci√≥n con tu LLM

- TBD

## üîú Conclusiones y siguientes pasos

- TBD

## More links

- <https://www.youtube.com/playlist?list=PLZVwXPbHD1KMRRUApy-cVI7q6B5QhoF3H>
- [Deploy ANY Open-Source LLM with Ollama on an AWS EC2 + GPU in 10 Min (Llama-3.1, Gemma-2 etc.)](https://www.youtube.com/watch?v=SAhUc9ywIiw)

## Feedback

- ["2-implementation-details"](https://github.com/CodelyTV/add_ai_follwing_best_practices-course/tree/main/02-software_architecture/2-implementation_details): no compila, faltan clases. Tambi√©n incluir info Makefile que facilite arrancarlo todo (e.g. que incluya un "docker compose up" para la DB, etc.)
