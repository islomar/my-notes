# Principles of real-time analytics on large datasets

- https://www.tinybird.co/courses/principles-of-real-time-analytics-contents
- ETA ~ 3 hours

## Why and intro to hardware

- https://www.youtube.com/watch?v=7nWwK829mZU
- The use case: working with large datasets.
- **Real time** is what we are used to, we use batch because we have no other option.
- Create real time simple models based on historical data. You would need to work with large datasets.
- "Tiempo real" es "que no te canses de esperar". Tal vez menos de 5 segundos.
- Modern computer HW. Sistemas donde podemos almacenar datos:
  - CPU (L1, L2, L3)
  - Memory
  - Disk
- **CPU speed**:
  - Today's CPUs: speed + parallelization
  - CPUs are much faster than memory
- **Memory speed**:
  - Latency vs Throughput
  - Example: GPU vs CPU
- **Mem throughput**
  - Cache L1 BW (bandwidth): 210 GB/s
  - Cache L2 BW (bandwidth): 80 GB/s
  - Cache L3 BW (bandwidth): 60 GB/s
  - RAM BW: 45 GB/s
- No nos importa demasiado la latencia, sí la velocidad
- **Memory speed**
  - Random access vs sequential access (5x)
  - Cached data 4x vs non cached data
- **Disk speed**
  - Disk is way slower than memory. Use SSD when possible.
- **The OS**
  - Intermediate layer
  - page cache // IO // threads
  - page cache importance
- The Cloud
  - Virtualization
  - Performance
  - Si la máquina está compartida, virtualizada de alguna manera, es probable que en algún momento una página salga de la cache y por tanto una acción dure mucho más de lo que pensábamos.

## Databases

- <https://www.youtube.com/watch?v=kjdk6z0b1jk>
- Use the right database for the job
- Different problems require different solutions.
- **What's a database?**
  - A file manager on stereoids.
  - A layer that abstracts data access from data storage
  - Why do we need this?
- **What's the right database for each job?**
  - Small app with relational model: SQLite might be the right choice.
  - Robust system that ensures data integrity: Postgres or MySQL could be your choice.
  - Store unstructured data: Mongo or Elastic sounds like the right choice.
  - High scalability: Cassandra, HBase, or any key value store are suitable.
  - You have 10 TB of data you need to analyze: ClickHouse, Spark, Druid.
  - All of them: forget about it, that's why you need to move data around (see GPU)
- We will focus on **analytical databases**: ClickHouse.
  - Normalmente enfocadas a añadir datos, NO a actualizarlos o eliminarlos (aunque se permita).
  - ¿Qué características tiene? El orden de los datos. En una DB "normal" se ordena por fila. En una DB analítica, se ordena por columnas.
  - **Row based vs Column based**
    - Just the data you need (in case you only need one column)
    - Better compression: los datos similares están en sitios muy cercanos, por lo que la compresión será mucho más sencilla.
    - Vectorization

## Source data and ingestion

- <https://youtu.be/KGO9hE33n7A>
- Before starting to query data, we need to get the data from the source and put it somewhere we can analyze it. This sounds easy but it's usually the hardest part of the job.
- Different **kind of source data**:
  - Streams/log based ones: for example, events generated by an application, Kafka, etc.
  - Data from transactional systems such as banks, e-commerce and most the web applications out there.
  - Raw data stored in files (CSV, JSON)
- Important **data source metrics**
  - Ingestion rate
  - Ingestion size
  - Example: bus fleet vs log file
  - The key is to **always do as much as possible at the same time**.
- **Data formats**
  - format: CSV/JSON/Parquet/SQLite/Avro/Arrow
  - storage: XML, JSON, JSONL, CSV, TSV, CSV.zip, CSV.gz
  - Bad: store large files, too small files, use zip, use any kind of compression that doesn't allow parallelization
  - Good: open formats, binary, medium size files business related partitioning.
- Structured vs unstructured
  - Unstructured means slower query times in general. That's why we are not talking about that there.
  - We usually store the raw data (e.g. get the JSON) and extract the data we need to query for different use cases (more about his on views)
- **Dimensions vs Events Data tables**
  - The common pattern Users vs Transactions:
    - Event data: large tables usually append only. E.g. bank transactions.
    - Dimensions: smaller (up to few millions). E.g. the customers/users.
  - In the bus example:
    - Bus position: event data
    - Line and driver properties: dimension data
- **Ingestion patterns: ways to send the data to the DB**
  - Source data storage: S3, transactional; Kafka.
  - Incremental, append-only (Kafka, S3)
  - Upsert (transactional)
  - Full replace (transactional, S3, usually for dimensions)
  - Partial replace: replace just a part of the data (a day)

## Storing data

- <https://youtu.be/u0LNX_mv4pY>
- Cuantos más datos a la vez, mejor
- **Layout**
  - Ingesting data != storing data
  - Storage is the key to getting fast queries
  - Organized by databases, tables, partitions, columns (usually)
- **Schemas**
  - Para analizar de manera muy rápida, hay que "dar por hecho" la mayor cantidad de cosas posibles. Por ejemplo, si vas a hacer una suma, "saber/asumir" que los datos son números "sumables".
  - In order to do **fast analysis** you need to know as much as possible about the data.
  - Data should be **homogeneous**
  - That's what a schema provides
- **All in memory**
  - Para trabajar en tiempo real tener los datos en memoria siempre es mejor
  - If you want to be fast it should be memory.
  - What if the dataset does not fit in memory? Compression, leverage caches, sharding, Pareto rule...
- **Network/HD speed vs CPU**
  - CPU is faster than network/memory/disk
  - What if we could transfer work from memory to CPU
  - Yes, compression. Compress all the things.
- **Compression**
  - Making it easy for the compression with codecs.
  - La compresión funciona bien según el tipo de dato.
  - Codecs and order
  - `[1,2,5,6,7,8,10,11,11]`
  - Deltas: `[1,1,3,1,1,1,2,1,0]`
- **Compression and codec types: when to use**
  - Hay muchos algoritmos de compresión, usar el adecuado según el caso de uso.
  - Ratio compression vs speed
  - Use cases for what they are optimized for:
    - LZ4: speed (it compresses 4x-6x)
    - ZSTD: compression rate
  - Las columnas que uses menos y que vayan a estar ocupando espacio en visto, usar un ZSTD.
  - Las que usas más, necesitas velocidad: LZ4.
- **Use the right type for the job**
  - Int64 vs Int8
  - Nullable vs non-nullable
    - When you use a nullable column, you are storing two values (it takes much more space and time).
    - Arrays
    - LowCardinality: tipos con pocas variaciones. E.g. si es un país, guardar un número que después en otro sitio se mapee.
    - Aggregation types
    - Bitmaps
- **Data sorting**
  - Or what's the data order in the disk/memory
  - In relational databases you don't usually "care" about this
  - But is this important? Yes, remember the memory access pattern speed.
  - There is no right order and there is no order that fits every use case.
  - Whit, what if I need to access data using different patterns.
  - You can store the same data sorted in different ways.
- **Indices for large datasets**
  - Tener orden no solo permite acceder de forma secuencial a los datos, sino que permite tener que leer datos. Ropa: si además de ordenar por tallas, tengo marcadores con las tallas, es más rápido encontrarlos.
  - Indices are book indices
  - Transactional databases are pretty common
  - Large systems usually base them on partitioning
  - rage based // block based
    - Normalmente son índices en árbol. Pero cuando hay muchos datos, es inmanejable.
    - Índices por rango: e.g. BRIN en Postgres (Block Range Index)
  - Other high selectivity indices
  - sometimes scannins is faster than index based access
- **Inserting + Storing**
  - Real time means running queries in less than Xa nd be able to see data with less than Y seconds
  - But if we add compression, codecs, sorting takes time.
  - We try to prepare everything outside of the database to be as fast as possible.

## Querying

- <https://youtu.be/i3VBgSmemNw>
- 42 minutes
- Se va a hablar de ClickHouse, pero los principios son los mismos para el resto.
- Partimos de que ya tenemos todo bien guardado y ordenado en la base de datos.
- How analytics databases work: parallelization, algorithms. Parallelization at many levels:
  - CPU: use different CPUs to process-splitting the load using the map-reduce algorithms
  - Vectorization: use SEE extension to process more than one value at a time
  - Machines: process data in several machines. Approximate data structures.

## Views

- <https://youtu.be/novN_Vcw0UI>
- 34 minutes

## Exposing the data

- TBD

## Scaling

- TBD
