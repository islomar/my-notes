# IA: Embeddings y RAG

- <https://pro.codely.com/library/ia-embeddings-y-rag-230838/655241/path/>
- <https://github.com/CodelyTV/ai-search_engine_with_rag-course>
- Asesor: √Ångel Delgado
- Duraci√≥n: ~ 3 horas

## üöÄ ¬øQu√© es RAG y los embeddings?, ¬øqu√© aprender√°s en el curso?

- **RAG (Retrieval-Augmented Generation)**
- 3 big LLM problems
  - Hallucinations
  - Cut-off date (no saben nada anterior a cierta fecha)
  - Finite context
- 3 ways to **RAG** (mecanismos para filtrar el conocimiento que le paso al prompt y limitar el contexto)
  - `SELECT * FROM courses WHERE category = 'DDD'`
  - Full-text search
    - e.g. con ElasticSearch, todos los cursos que contienen la palabra "Intro"
  - Vector search
    - e.g. "l√≠stame los cursos introductorios de arquitectura y dise√±o de software". No contiene la palabra "intro" necesariamente, e.g. el de "Clean Code".
    - Soluci√≥n: b√∫squeda vectorial aka b√∫squeda sem√°ntica.
    - `SELECT * FROM courses ORDER BY (embedding <=> ${generateEmbedding("Intro a Arq. y dise√±o de software")}) LIMIT 10;`

### üßÆ Qu√© es una b√∫squeda sem√°ntica y qu√© son los embeddings

- Gato, gato negro, perro, perro negro
- Una dimensi√≥n (tipo de animal):           vector(1)
- Dos dimensiones (tipo de animal y color): vector(2)
  - E.g. El gato est√° en la [-1, 1], el gato negro en la [-1, -1]
- Un **embedding** es un vector de N dimensiones (N es del orden de cientos o miles).
  - Esas dimensiones, inicialmente, ten√≠an connotaciones de significado concreto. A d√≠a de hoy no, son conceptos abstractos.
  - Potencia de 2
  - A m√°s dimensiones, m√°s lenta es la b√∫squeda
- Se mide la "cercan√≠a" teniendo en cuenta el √°ngulo del vector desde lo que buscas a lo que existe (NO por distancia euclidiana)
- Existen **modelos de embeddings**: le pasas un texto y te lo pasa a un embedding, e.g. "perro muy negro"
- La **comparaci√≥n** del embedding se har√≠a en la propia base de datos, pero la **generaci√≥n** puede ser en la DB o llamando a un LLM o...
- **Embedding models** (to generate the embedding/vector)
  - `nomic-embed-text` vector(768): te lo puedes montar en local con OLlamA, <https://ollama.com/library/nomic-embed-text>
  - `text-embedding-3-small` vector(1536): de OpenAI
  - `text-embedding-3-large` vector(3072): de OpenAI
- [Recomendaci√≥n de Supabase](https://supabase.com/docs/guides/ai/vector-columns#create-a-table-to-store-vectors), recomienda vector(384)
  - Para el embedding usan el [modelo `gte-small`](https://huggingface.co/Supabase/gte-small): en la tabla aparecen las dimensiones.

### üßë‚Äçüè´ Implementa RAG b√°sico con PgVector + Langchain: B√∫squeda de cursos similares

- [C√≥digo de ejemplo: RAG con PgVector](https://github.com/CodelyTV/ai-search_engine_with_rag-course/tree/main/01-what_is_rag/3-implement_rag_pgvector)
- `pgvector` es la extensi√≥n de Postgres que permite hacer b√∫squedas vectoriales dentro de Postgres
  - NO se encarga de generar el embedding/vector, sino de calcular la distancia. Y tambi√©n permite almacenar el vector.
- A tu tabla normal, e.g. `cursos`, se le a√±ade un campo `embedding vector(768)`
- En este ejemplo, s√≥lo usa el nombre del curso para generar el embedding (usando OllamaEmbeddings, ofrecido por LangChain)
  - Tenemos que pasar el embedding a JSon antes de guardarlo en la tabla.
- De momento, una b√∫squeda por "Introducci√≥n a DDD" no est√° fina.

## üî§ Embeddings: B√∫squeda de curso por significado

### üê≥ Las diferentes formas de a√±adir embeddings

- En algunos tutoriales se propone tener una tabla con todos los embeddings, no tenerlos en las propias tablas de negocios.
  - Pero es m√°s sencillo empezar por a√±adir el campo `embedding` a la tabla
- Las diferentes formas de a√±adir embeddings
  - **Specific field**
    - A la tabla `courses` le a√±ado un campo `embedding`
  - **Specific table**
    - Tengo una tabla`course_embeddings` (desnormalizaci√≥n)
    - Probablemente la mejor.
  - **Generic table** (con todos los embeddings dentro)
    - Tengo una tabla gen√©rica `embeddings`
    - IMO demasiada abstracci√≥n, demasiada generalizaci√≥n

![](ai-embeddings-and-rag-codely/embeddings_table_comparisons.png)

### üì§ Contexto: C√≥mo es una aplicaci√≥n EDA con sugerencias LLM

- Overview del curso "Integra IA siguiendo buenas pr√°cticas"

### ‚ûï A√±ade RAG a una aplicaci√≥n EDA existente

- [Code example: add RAG existing embedding](https://github.com/CodelyTV/ai-search_engine_with_rag-course/tree/main/02-embedding/3-add_rag_existing_application)
  - The first execution failed because we were sending too much context. We can solve it using RAG.
- With RAG we limit the courses for which we want to generate recommendations
  - [We would ask for "similar" user courses, not all the user courses](https://github.com/CodelyTV/ai-search_engine_with_rag-course/blob/c966d5ac6c5d21d6eafbc7932a36a7ec3fcf61b4/02-embedding/3-add_rag_existing_application/src/contexts/mooc/user-course-suggestions/infrastructure/OllamaLlama31CourseSuggestionsGenerator.ts#L34-L34)
  - [`searchSimilar()`](https://github.com/CodelyTV/ai-search_engine_with_rag-course/blob/c966d5ac6c5d21d6eafbc7932a36a7ec3fcf61b4/02-embedding/3-add_rag_existing_application/src/contexts/mooc/courses/infrastructure/PostgresCourseRepository.ts#L75-L75)

```javascript
const availableCourses = await this.courseRepository.searchAll();
// const availableCourses = await
// this.courseRepository.searchSimilar(completedCourseIds);
```

- No hay determinismo, podr√≠a fallar: implementar por ejemplo _retries_

## üë®‚Äçüíª Optimiza tu RAG a√±adiendo m√°s contexto

### üõñ Qu√© valores guardar como embedding
- [Ejemplo de c√≥digo: qu√© guardar en el embedding](https://github.com/CodelyTV/ai-search_engine_with_rag-course/tree/main/03-add_rag/1-what_to_store_embedding)
- A la hora de crear el embedding, [a√±adimos m√°s informaci√≥n](https://github.com/CodelyTV/ai-search_engine_with_rag-course/blob/c966d5ac6c5d21d6eafbc7932a36a7ec3fcf61b4/03-add_rag/1-what_to_store_embedding/src/app/scripts/search-courses-by-ids.ts#L30-L30): guardar no √∫nicamente el nombre del curso, sino tambi√©n un resumen y las categor√≠as. De esa manera, obtengo una informaci√≥n m√°s precisa.
- [LangSmith](https://www.langchain.com/langsmith): observabilidad existente en LangChain
  - LangSmith is a unified observability & evals platform where teams can debug, test, and monitor AI app performance ‚Äî whether building with LangChain or not.

### üó£Ô∏è A√±ade m√°s contexto en los embeddings: B√∫squedas granulares
- [C√≥digo de ejemplo: c√≥mo a√±adir m√°s contexto en los embeddings](https://github.com/CodelyTV/ai-search_engine_with_rag-course/tree/main/03-add_rag/2-add_embedding_context)
- Para el embedding, [a√±adimos el ID, nombre, summary y categor√≠as](https://github.com/CodelyTV/ai-search_engine_with_rag-course/blob/c966d5ac6c5d21d6eafbc7932a36a7ec3fcf61b4/03-add_rag/2-add_embedding_context/src/contexts/mooc/user-course-suggestions/infrastructure/OllamaLlama31CourseSuggestionsGenerator.ts#L108-L108), en formato `yaml` (porque cuantos menos caracteres metamos para el embedding, mejor, si no se creer√° que importan - mejor no usar json o XML para esto)

### ‚è≥ Ordena los resultados de una b√∫squeda por embeddings
- Interesa hacerlo por ejemplo si queremos los N cursos similares m√°s recientes, por orden.
- [C√≥digo de ejemplo: ordenaci√≥n RAG m√°s reciente](https://github.com/CodelyTV/ai-search_engine_with_rag-course/tree/main/03-add_rag/3-rag_sort_by_recent)
- [En la SQL, ordenamos por la combinaci√≥n del embedding y la recencia (con un peso, 0.001, por prueba y error, se puede empezar por 0.1)](https://github.com/CodelyTV/ai-search_engine_with_rag-course/blob/c966d5ac6c5d21d6eafbc7932a36a7ec3fcf61b4/03-add_rag/3-rag_sort_by_recent/src/contexts/mooc/courses/infrastructure/PostgresCourseRepository.ts#L86-L86)

## üî• Qu√© base de datos elegir para hacer b√∫squeda por embeddings

### ‚õìÔ∏è C√≥mo delegar a infraestructura la generaci√≥n de embeddings
- TBD

## üçΩÔ∏è Ingesta en base de datos vectorial datos de terceros

- TBD

## üçÑ Mejora la b√∫squeda por embeddings con datos de terceros

- TBD

## üîú Conclusiones y siguientes pasos

- [ChatGPT Prompting vs RAG vs fine tuning: Aprovecha la IA al m√°ximo | #laFunci√≥n 9x18](https://www.youtube.com/watch?v=bjCdsnkQ6Dw)
- M√°s cursos
  - [Embeddings autom√°ticos en Postgres](https://pro.codely.com/library/embeddings-automaticos-en-postgres-236271/702554/about/)
