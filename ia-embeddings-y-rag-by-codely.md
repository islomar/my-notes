# IA: Embeddings y RAG

- https://pro.codely.com/library/ia-embeddings-y-rag-230838/655241/path/
- https://github.com/CodelyTV/ai-search_engine_with_rag-course
- Asesor: √Ångel Delgado
- Duraci√≥n: ~ 3 horas

## üöÄ ¬øQu√© es RAG y los embeddings?, ¬øqu√© aprender√°s en el curso?
- RAG (Retrieval-Augmented Generation)
- 3 big LLM problems
  - Hallucinations
  - Cut-off date (no saben nada anterior a cierta fecha)
  - Finite context
- 3 ways to RAG (mecanismos para filtrar el conocimiento que le paso al prompt y limitar el contexto)
  - `SELECT * FROM courses WHERE category = 'DDD'`
  - Full-text search
    - e.g. con ElasticSearch, todos los cursos que contienen la palabra "Intro"
  - Vector search
    - e.g. "l√≠stame los cursos introductorios de arquitectura y dise√±o de software". No contiene la palabra "intro" necesariamente, e.g. el de "Clean Code".
    - Soluci√≥n: b√∫squeda vectorial aka b√∫squeda sem√°ntica.
    - `SELECT * FROM courses ORDER BY (embedding <=> ${generateEmbedding("Intro a Arq. y dise√±o de software")}) LIMIT 10;`

### üßÆ Qu√© es una b√∫squeda sem√°ntica y qu√© son los embeddings
- Gato, gato negro, perro, perro negro
- Una dimensi√≥n (tipo de animal):           vector(1)
- Dos dimensiones (tipo de animal y color): vector(2)
  - E.g. El gato est√° en la [-1, 1], el gato negro en la [-1, -1]
- Un **embedding** es un vector de N dimensiones (N es del orden de cientos o miles).
  - Esas dimensiones, inicialmente, ten√≠an connotaciones de significado concreto. A d√≠a de hoy no, son conceptos abstractos.
  - Potencia de 2
  - A m√°s dimensiones, m√°s lenta es la b√∫squeda
- Se mide la "cercan√≠a" teniendo en cuenta el √°ngulo del vector desde lo que buscas a lo que existe (NO por distancia euclidiana)
- Existen **modelos de embeddings**: le pasas un texto y te lo pasa a un embedding, e.g. "perro muy negro"
- La **comparaci√≥n** del embedding se har√≠a en la propia base de datos, pero la **generaci√≥n** puede ser en la DB o llamando a un LLM o...
- **Embedding models** (to generate the embedding/vector)
  - `nomic-embed-text` vector(768): te lo puedes montar en local con OLlamA, https://ollama.com/library/nomic-embed-text
  - `text-embedding-3-small` vector(1536): de OpenAI
  - `text-embedding-3-large` vector(3072): de OpenAI
- [Recomendaci√≥n de Supabase](https://supabase.com/docs/guides/ai/vector-columns#create-a-table-to-store-vectors), recomienda vector(384)
  - Para el embedding usan el [modelo `gte-small`](https://huggingface.co/Supabase/gte-small): en la tabla aparecen las dimensiones.

### üßë‚Äçüè´ Implementa RAG b√°sico con PgVector + Langchain: B√∫squeda de cursos similares
- [C√≥digo de ejemplo: RAG con PgVector](https://github.com/CodelyTV/ai-search_engine_with_rag-course/tree/main/01-what_is_rag/3-implement_rag_pgvector)
- `pgvector` es la extensi√≥n de Postgres que permite hacer b√∫squedas vectoriales dentro de Postgres
  - NO se encarga de generar el embedding/vector, sino de calcular la distancia. Y tambi√©n permite almacenar el vector.
- A tu tabla normal, e.g. `cursos`, se le a√±ade un campo `embedding vector(768)`
- En este ejemplo, s√≥lo usa el nombre del curso para generar el embedding (usando OllamaEmbeddings, ofrecido por LangChain)
  - Tenemos que pasar el embedding a JSon antes de guardarlo en la tabla.
- De momento, una b√∫squeda por "Introducci√≥n a DDD" no est√° fina.

## üî§ Embeddings: B√∫squeda de curso por significado
- En algunos tutoriales se propone tener una tabla con todos los embeddings, no tenerlos en las propias tablas de negocios.
  - Pero es m√°s sencillo empezar por a√±adir el campo `embedding` a la tabla
- Las diferentes formas de a√±adir embeddings
  - **Specific field**
    - A la tabla `courses` le a√±ado un campo `embedding`
  - **Specific table**
    - Tengo una tabla`course_embeddings` (desnormalizaci√≥n)
    - Probablemente la mejor.
  - **Generic table** (con todos los embeddings dentro)
    - Tengo una tabla gen√©rica `embeddings`
    - IMO demasiada abstracci√≥n, demasiada generalizaci√≥n

## üë®‚Äçüíª Optimiza tu RAG a√±adiendo m√°s contexto
- TBD

## üî• Qu√© base de datos elegir para hacer b√∫squeda por embeddings
- TBD

## üçΩÔ∏è Ingesta en base de datos vectorial datos de terceros
- TBD

## üçÑ Mejora la b√∫squeda por embeddings con datos de terceros
- TBD

## üîú Conclusiones y siguientes pasos
- [ChatGPT Prompting vs RAG vs fine tuning: Aprovecha la IA al m√°ximo | #laFunci√≥n 9x18](https://www.youtube.com/watch?v=bjCdsnkQ6Dw)
- M√°s cursos
  - [Embeddings autom√°ticos en Postgres](https://pro.codely.com/library/embeddings-automaticos-en-postgres-236271/702554/about/)
